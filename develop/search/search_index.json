{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Quickstart","text":"<p>Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use.</p>"},{"location":"#install","title":"Install","text":"<p>You can install and use <code>trackers</code> in a Python&gt;=3.10 environment. For detailed installation instructions, including installing from source and setting up a local development environment, check out our install page.</p> <p>Installation</p> <p> </p> pipuv <pre><code>pip install trackers\n</code></pre> <pre><code>uv pip install trackers\n</code></pre>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li> <p>How to Track Objects with SORT</p> <p></p> <p>End-to-end example showing how to run RF-DETR detection with the SORT tracker.</p> <p> Run Google Colab</p> </li> <li> <p>How to Track Objects with ByteTrack</p> <p></p> <p>End-to-end example showing how to run RF-DETR detection with the ByteTrack tracker.</p> <p> Run Google Colab</p> </li> </ul>"},{"location":"#tracking-algorithms","title":"Tracking Algorithms","text":"<p>Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms. The package currently supports SORT and ByteTrack. OC-SORT support is coming soon. For comparisons, see the tracker comparison page.</p> Algorithm Trackers API MOT17 HOTA MOT17 IDF1 MOT17 MOTA SportsMOT HOTA SoccerNet HOTA SORT <code>SORTTracker</code> 58.4 69.9 67.2 70.9 81.6 ByteTrack <code>ByteTrackTracker</code> 60.1 73.2 74.1 73.0 84.0 OC-SORT <code>OCSORTTracker</code> \u2014 \u2014 \u2014 \u2014 \u2014"},{"location":"#quickstart","title":"Quickstart","text":"<p>With a modular design, Trackers lets you combine object detectors from different libraries with the tracker of your choice. Here's how you can use ByteTrack with various detectors. These examples use OpenCV for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code> with your input.</p> RF-DETRInferenceUltralyticsTransformers <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = get_model(model_id=\"rfdetr-medium\")\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    result = model.infer(frame_rgb)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"Inference + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = YOLO(\"yolo26m.pt\")\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    result = model(frame_rgb)[0]\n    detections = sv.Detections.from_ultralytics(result)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"Ultralytics + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import torch\nimport cv2\nimport supervision as sv\nfrom trackers import ByteTrackTracker\nfrom transformers import RTDetrImageProcessor, RTDetrV2ForObjectDetection\n\ntracker = ByteTrackTracker()\nprocessor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\nmodel = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    inputs = processor(images=frame_rgb, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    h, w = frame_bgr.shape[:2]\n    results = processor.post_process_object_detection(\n        outputs, target_sizes=torch.tensor([[h, w]]), threshold=0.5\n    )[0]\n\n    detections = sv.Detections.from_transformers(\n        transformers_results=results, id2label=model.config.id2label\n    )\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"Transformers + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"api-evals/","title":"Evals API","text":""},{"location":"api-evals/#trackers.eval.evaluate.evaluate_mot_sequence","title":"<code>trackers.eval.evaluate.evaluate_mot_sequence(gt_path, tracker_path, metrics=None, threshold=0.5)</code>","text":"<p>Evaluate a single multi-object tracking result against ground truth. Computes standard multi-object tracking metrics (CLEAR MOT, HOTA, Identity) for one sequence by matching predicted tracks to ground-truth tracks using per-frame IoU (Intersection over Union).</p> <p>TrackEval parity</p> <p>This evaluation code is intentionally designed to match the core matching logic and metric calculations of TrackEval.</p> <p>Parameters:</p> Name Type Description Default <code>gt_path</code> <code>str | Path</code> <p>Path to the ground-truth MOT file.</p> required <code>tracker_path</code> <code>str | Path</code> <p>Path to the tracker MOT file.</p> required <code>metrics</code> <code>list[str] | None</code> <p>Metric families to compute. Supported values are <code>[\"CLEAR\", \"HOTA\", \"Identity\"]</code>. Defaults to <code>[\"CLEAR\"]</code>.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>IoU threshold for <code>CLEAR</code> and <code>Identity</code> matching. Defaults to <code>0.5</code>. <code>HOTA</code> evaluates across multiple thresholds internally.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>SequenceResult</code> <p><code>SequenceResult</code> with <code>CLEAR</code>, <code>HOTA</code>, and/or <code>Identity</code> populated based on <code>metrics</code>.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>gt_path</code> or <code>tracker_path</code> does not exist.</p> <code>ValueError</code> <p>If an unsupported metric family is requested.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from trackers.eval import evaluate_mot_sequence\n\n&gt;&gt;&gt; result = evaluate_mot_sequence(\n...     gt_path=\"data/gt/MOT17-02/gt.txt\",\n...     tracker_path=\"data/trackers/MOT17-02.txt\",\n...     metrics=[\"CLEAR\", \"HOTA\", \"Identity\"],\n... )\n\n&gt;&gt;&gt; print(result.CLEAR.MOTA)\n# 0.756\n\n&gt;&gt;&gt; print(result.table(columns=[\"MOTA\", \"HOTA\", \"IDF1\", \"IDSW\"]))\n# Sequence                           MOTA    HOTA    IDF1  IDSW\n# -------------------------------------------------------------\n# MOT17-02                         75.600  62.300  72.100    42\n</code></pre>"},{"location":"api-evals/#trackers.eval.evaluate.evaluate_mot_sequences","title":"<code>trackers.eval.evaluate.evaluate_mot_sequences(gt_dir, tracker_dir, seqmap=None, metrics=None, threshold=0.5, benchmark=None, split=None, tracker_name=None)</code>","text":"<p>Evaluate multiple multi-object tracking results against ground truth. Computes standard multi-object tracking metrics (CLEAR MOT, HOTA, Identity) across one or more sequences by matching predicted tracks to ground-truth tracks using per-frame IoU (Intersection over Union). Returns both per-sequence and aggregated (combined) results.</p> <p>TrackEval parity</p> <p>This evaluation code is intentionally designed to match the core matching logic and metric calculations of TrackEval.</p> <p>Supported dataset layouts</p> MOT layoutFlat layout <pre><code>gt_dir/\n\u2514\u2500\u2500 MOT17-train/\n    \u251c\u2500\u2500 MOT17-02-FRCNN/\n    \u2502   \u2514\u2500\u2500 gt/gt.txt\n    \u251c\u2500\u2500 MOT17-04-FRCNN/\n    \u2502   \u2514\u2500\u2500 gt/gt.txt\n    \u251c\u2500\u2500 MOT17-05-FRCNN/\n    \u2502   \u2514\u2500\u2500 gt/gt.txt\n    \u2514\u2500\u2500 ...\n\ntracker_dir/\n\u2514\u2500\u2500 MOT17-train/\n    \u2514\u2500\u2500 ByteTrack/\n        \u2514\u2500\u2500 data/\n            \u251c\u2500\u2500 MOT17-02-FRCNN.txt\n            \u251c\u2500\u2500 MOT17-04-FRCNN.txt\n            \u251c\u2500\u2500 MOT17-05-FRCNN.txt\n            \u2514\u2500\u2500 ...\n</code></pre> <pre><code>gt_dir/\n\u251c\u2500\u2500 MOT17-02.txt\n\u251c\u2500\u2500 MOT17-04.txt\n\u251c\u2500\u2500 MOT17-05.txt\n\u2514\u2500\u2500 ...\n\ntracker_dir/\n\u251c\u2500\u2500 MOT17-02.txt\n\u251c\u2500\u2500 MOT17-04.txt\n\u251c\u2500\u2500 MOT17-05.txt\n\u2514\u2500\u2500 ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gt_dir</code> <code>str | Path</code> <p>Directory with ground-truth files.</p> required <code>tracker_dir</code> <code>str | Path</code> <p>Directory with tracker prediction files.</p> required <code>seqmap</code> <code>str | Path | None</code> <p>Optional sequence map. If provided, only those sequences are evaluated.</p> <code>None</code> <code>metrics</code> <code>list[str] | None</code> <p>Metric families to compute. Supported values are <code>[\"CLEAR\", \"HOTA\", \"Identity\"]</code>. Defaults to <code>[\"CLEAR\"]</code>.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>IoU threshold for <code>CLEAR</code> and <code>Identity</code>. Defaults to <code>0.5</code>.</p> <code>0.5</code> <code>benchmark</code> <code>str | None</code> <p>Override auto-detected benchmark name (e.g., <code>\"MOT17\"</code>).</p> <code>None</code> <code>split</code> <code>str | None</code> <p>Override auto-detected split name (e.g., <code>\"train\"</code>, <code>\"val\"</code>).</p> <code>None</code> <code>tracker_name</code> <code>str | None</code> <p>Override auto-detected tracker name.</p> <code>None</code> <p>Returns:</p> Type Description <code>BenchmarkResult</code> <p><code>BenchmarkResult</code> with per-sequence results and a <code>COMBINED</code> aggregate.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>gt_dir</code> or <code>tracker_dir</code> does not exist.</p> <code>ValueError</code> <p>If auto-detection finds multiple valid options.</p> <p>Examples:</p> <p>Auto-detect layout and evaluate all sequences:</p> <pre><code>&gt;&gt;&gt; from trackers.eval import evaluate_mot_sequences\n\n&gt;&gt;&gt; result = evaluate_mot_sequences(\n...     gt_dir=\"data/gt/\",\n...     tracker_dir=\"data/trackers/\",\n...     metrics=[\"CLEAR\", \"HOTA\", \"Identity\"],\n... )\n\n&gt;&gt;&gt; print(result.table(columns=[\"MOTA\", \"HOTA\", \"IDF1\", \"IDSW\"]))\n# Sequence                           MOTA    HOTA    IDF1  IDSW\n# -------------------------------------------------------------\n# sequence1                        74.800  60.900  71.200    37\n# sequence2                        76.100  63.200  72.500    45\n# -------------------------------------------------------------\n# COMBINED                         75.450  62.050  71.850    82\n</code></pre>"},{"location":"api-evals/#trackers.eval.results.SequenceResult","title":"<code>trackers.eval.results.SequenceResult</code>  <code>dataclass</code>","text":"<p>Result for a single sequence evaluation.</p> <p>Attributes:</p> Name Type Description <code>sequence</code> <code>str</code> <p>Name of the sequence.</p> <code>CLEAR</code> <code>CLEARMetrics | None</code> <p>CLEAR metrics for this sequence, or <code>None</code> if not requested.</p> <code>HOTA</code> <code>HOTAMetrics | None</code> <p>HOTA metrics for this sequence, or <code>None</code> if not requested.</p> <code>Identity</code> <code>IdentityMetrics | None</code> <p>Identity metrics for this sequence, or <code>None</code> if not requested.</p>"},{"location":"api-evals/#trackers.eval.results.SequenceResult.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>SequenceResult</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with sequence name and metrics.</p> required <p>Returns:</p> Type Description <code>SequenceResult</code> <p><code>SequenceResult</code> instance.</p>"},{"location":"api-evals/#trackers.eval.results.SequenceResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api-evals/#trackers.eval.results.SequenceResult.json","title":"<code>json(indent=2)</code>","text":"<p>Serialize to JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int</code> <p>Indentation level for formatting. Defaults to <code>2</code>.</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation.</p>"},{"location":"api-evals/#trackers.eval.results.SequenceResult.table","title":"<code>table(columns=None)</code>","text":"<p>Format as a table string.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str] | None</code> <p>Metric columns to include. If <code>None</code>, includes all available metrics.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted table string.</p>"},{"location":"api-evals/#trackers.eval.results.BenchmarkResult","title":"<code>trackers.eval.results.BenchmarkResult</code>  <code>dataclass</code>","text":"<p>Result for multi-sequence evaluation.</p> <p>Attributes:</p> Name Type Description <code>sequences</code> <code>dict[str, SequenceResult]</code> <p>Dictionary mapping sequence names to their results.</p> <code>aggregate</code> <code>SequenceResult</code> <p>Combined metrics across all sequences.</p>"},{"location":"api-evals/#trackers.eval.results.BenchmarkResult.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>BenchmarkResult</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with sequences and aggregate results.</p> required <p>Returns:</p> Type Description <code>BenchmarkResult</code> <p><code>BenchmarkResult</code> instance.</p>"},{"location":"api-evals/#trackers.eval.results.BenchmarkResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api-evals/#trackers.eval.results.BenchmarkResult.json","title":"<code>json(indent=2)</code>","text":"<p>Serialize to JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int</code> <p>Indentation level for formatting. Defaults to <code>2</code>.</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation.</p>"},{"location":"api-evals/#trackers.eval.results.BenchmarkResult.table","title":"<code>table(columns=None)</code>","text":"<p>Format as a table string.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str] | None</code> <p>Metric columns to include. If <code>None</code>, includes all available metrics.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted table string.</p>"},{"location":"api-evals/#trackers.eval.results.BenchmarkResult.save","title":"<code>save(path)</code>","text":"<p>Save to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Destination file path.</p> required"},{"location":"api-evals/#trackers.eval.results.BenchmarkResult.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Source file path.</p> required <p>Returns:</p> Type Description <code>BenchmarkResult</code> <p><code>BenchmarkResult</code> instance.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p>"},{"location":"api-evals/#trackers.eval.results.CLEARMetrics","title":"<code>trackers.eval.results.CLEARMetrics</code>  <code>dataclass</code>","text":"<p>CLEAR metrics with TrackEval-compatible field names. Float metrics are stored as fractions (0-1 range), not percentages. The values follow the original CLEAR MOT definitions.</p> <p>Attributes:</p> Name Type Description <code>MOTA</code> <code>float</code> <p>Multiple Object Tracking Accuracy. Penalizes false negatives, false positives, and ID switches: <code>(TP - FP - IDSW) / (TP + FN)</code>. Can be negative when errors exceed matches.</p> <code>MOTP</code> <code>float</code> <p>Multiple Object Tracking Precision. Mean IoU of matched pairs. Measures localization quality only.</p> <code>MODA</code> <code>float</code> <p>Multiple Object Detection Accuracy. Like MOTA but ignores ID switches: <code>(TP - FP) / (TP + FN)</code>.</p> <code>CLR_Re</code> <code>float</code> <p>CLEAR recall. Fraction of GT detections matched: <code>TP / (TP + FN)</code>.</p> <code>CLR_Pr</code> <code>float</code> <p>CLEAR precision. Fraction of tracker detections correct: <code>TP / (TP + FP)</code>.</p> <code>MTR</code> <code>float</code> <p>Mostly tracked ratio. Fraction of GT tracks tracked for &gt;80% of their lifespan.</p> <code>PTR</code> <code>float</code> <p>Partially tracked ratio. Fraction of GT tracks tracked for 20-80%.</p> <code>MLR</code> <code>float</code> <p>Mostly lost ratio. Fraction of GT tracks tracked for &lt;20%.</p> <code>sMOTA</code> <code>float</code> <p>Summed MOTA. Replaces TP count with IoU sum: <code>(MOTP_sum - FP - IDSW) / (TP + FN)</code>.</p> <code>CLR_TP</code> <code>int</code> <p>True positives. Number of correct matches.</p> <code>CLR_FN</code> <code>int</code> <p>False negatives. Number of missed GT detections.</p> <code>CLR_FP</code> <code>int</code> <p>False positives. Number of spurious tracker detections.</p> <code>IDSW</code> <code>int</code> <p>ID switches. Times a GT track changes its matched tracker ID.</p> <code>MT</code> <code>int</code> <p>Mostly tracked count. Number of GT tracks tracked &gt;80%.</p> <code>PT</code> <code>int</code> <p>Partially tracked count. Number of GT tracks tracked 20-80%.</p> <code>ML</code> <code>int</code> <p>Mostly lost count. Number of GT tracks tracked &lt;20%.</p> <code>Frag</code> <code>int</code> <p>Fragmentations. Times a tracked GT becomes untracked then tracked again.</p> <code>MOTP_sum</code> <code>float</code> <p>Raw IoU sum for aggregation across sequences.</p> <code>CLR_Frames</code> <code>int</code> <p>Number of frames evaluated.</p>"},{"location":"api-evals/#trackers.eval.results.CLEARMetrics.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>CLEARMetrics</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with metric values.</p> required <p>Returns:</p> Type Description <code>CLEARMetrics</code> <p><code>CLEARMetrics</code> instance.</p>"},{"location":"api-evals/#trackers.eval.results.CLEARMetrics.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api-evals/#trackers.eval.results.HOTAMetrics","title":"<code>trackers.eval.results.HOTAMetrics</code>  <code>dataclass</code>","text":"<p>HOTA metrics with TrackEval-compatible field names. HOTA evaluates both detection quality and association quality. Float metrics are stored as fractions (0-1 range).</p> <p>Attributes:</p> Name Type Description <code>HOTA</code> <code>float</code> <p>Higher Order Tracking Accuracy. Geometric mean of DetA and AssA, averaged over 19 IoU thresholds (0.05 to 0.95).</p> <code>DetA</code> <code>float</code> <p>Detection accuracy: <code>TP / (TP + FN + FP)</code>.</p> <code>AssA</code> <code>float</code> <p>Association accuracy for matched detections over time.</p> <code>DetRe</code> <code>float</code> <p>Detection recall: <code>TP / (TP + FN)</code>.</p> <code>DetPr</code> <code>float</code> <p>Detection precision: <code>TP / (TP + FP)</code>.</p> <code>AssRe</code> <code>float</code> <p>Association recall. For each GT ID, measures how consistently it maps to a single tracker ID across time.</p> <code>AssPr</code> <code>float</code> <p>Association precision. For each tracker ID, measures how consistently it maps to a single GT ID across time.</p> <code>LocA</code> <code>float</code> <p>Localization accuracy. Mean IoU for matched pairs.</p> <code>OWTA</code> <code>float</code> <p>Open World Tracking Accuracy. <code>sqrt(DetRe * AssA)</code>, useful when precision is less meaningful.</p> <code>HOTA_TP</code> <code>int</code> <p>True positive count summed over all 19 thresholds.</p> <code>HOTA_FN</code> <code>int</code> <p>False negative count summed over all 19 thresholds.</p> <code>HOTA_FP</code> <code>int</code> <p>False positive count summed over all 19 thresholds.</p>"},{"location":"api-evals/#trackers.eval.results.HOTAMetrics.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>HOTAMetrics</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with metric values.</p> required <p>Returns:</p> Type Description <code>HOTAMetrics</code> <p><code>HOTAMetrics</code> instance.</p>"},{"location":"api-evals/#trackers.eval.results.HOTAMetrics.to_dict","title":"<code>to_dict(include_arrays=False, arrays_as_list=True)</code>","text":"<p>Convert to dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>include_arrays</code> <code>bool</code> <p>Whether to include per-alpha arrays. Defaults to <code>False</code>.</p> <code>False</code> <code>arrays_as_list</code> <code>bool</code> <p>Whether to convert arrays to lists for JSON serialization. Defaults to <code>True</code>. Set to <code>False</code> to keep numpy arrays.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api-evals/#trackers.eval.results.IdentityMetrics","title":"<code>trackers.eval.results.IdentityMetrics</code>  <code>dataclass</code>","text":"<p>Identity metrics with TrackEval-compatible field names. Identity metrics measure global ID consistency using an optimal one-to-one assignment between GT and tracker IDs across the full sequence.</p> <p>Attributes:</p> Name Type Description <code>IDF1</code> <code>float</code> <p>ID F1 score. Harmonic mean of IDR and IDP, the primary identity metric.</p> <code>IDR</code> <code>float</code> <p>ID recall. <code>IDTP / (IDTP + IDFN)</code>, fraction of GT detections with correct global ID assignment.</p> <code>IDP</code> <code>float</code> <p>ID precision. <code>IDTP / (IDTP + IDFP)</code>, fraction of tracker detections with correct global ID assignment.</p> <code>IDTP</code> <code>int</code> <p>ID true positives. Detections matched with globally consistent IDs.</p> <code>IDFN</code> <code>int</code> <p>ID false negatives. GT detections not matched or matched to the wrong global ID.</p> <code>IDFP</code> <code>int</code> <p>ID false positives. Tracker detections not matched or matched to the wrong global ID.</p>"},{"location":"api-evals/#trackers.eval.results.IdentityMetrics.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>IdentityMetrics</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with metric values.</p> required <p>Returns:</p> Type Description <code>IdentityMetrics</code> <p><code>IdentityMetrics</code> instance.</p>"},{"location":"api-evals/#trackers.eval.results.IdentityMetrics.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api-trackers/","title":"Trackers API","text":""},{"location":"api-trackers/#sort","title":"SORT","text":""},{"location":"api-trackers/#trackers.core.sort.tracker.SORTTracker","title":"<code>trackers.core.sort.tracker.SORTTracker</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Track objects using SORT algorithm with Kalman filter and IoU matching. Provides simple and fast online tracking using only bounding box geometry without appearance features.</p> <p>Parameters:</p> Name Type Description Default <code>lost_track_buffer</code> <code>int</code> <p><code>int</code> specifying number of frames to buffer when a track is lost. Increasing this value enhances occlusion handling but may increase ID switching for similar objects.</p> <code>30</code> <code>frame_rate</code> <code>float</code> <p><code>float</code> specifying video frame rate in frames per second. Used to scale the lost track buffer for consistent tracking across different frame rates.</p> <code>30.0</code> <code>track_activation_threshold</code> <code>float</code> <p><code>float</code> specifying minimum detection confidence to create new tracks. Higher values reduce false positives but may miss low-confidence objects.</p> <code>0.25</code> <code>minimum_consecutive_frames</code> <code>int</code> <p><code>int</code> specifying number of consecutive frames before a track is considered valid. Before reaching this threshold, tracks are assigned <code>tracker_id</code> of <code>-1</code>.</p> <code>3</code> <code>minimum_iou_threshold</code> <code>float</code> <p><code>float</code> specifying IoU threshold for associating detections to existing tracks. Higher values require more overlap.</p> <code>0.3</code>"},{"location":"api-trackers/#trackers.core.sort.tracker.SORTTracker.update","title":"<code>update(detections)</code>","text":"<p>Update tracker state with new detections and return tracked objects. Performs Kalman filter prediction, IoU-based association, and initializes new tracks for unmatched high-confidence detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p><code>sv.Detections</code> containing bounding boxes with shape <code>(N, 4)</code> in <code>(x_min, y_min, x_max, y_max)</code> format and optional confidence scores.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p><code>sv.Detections</code> with <code>tracker_id</code> assigned for each detection. Unmatched or immature tracks have <code>tracker_id</code> of <code>-1</code>.</p>"},{"location":"api-trackers/#trackers.core.sort.tracker.SORTTracker.reset","title":"<code>reset()</code>","text":"<p>Reset tracker state by clearing all tracks and resetting ID counter. Call this method when switching to a new video or scene.</p>"},{"location":"api-trackers/#bytetrack","title":"ByteTrack","text":""},{"location":"api-trackers/#trackers.core.bytetrack.tracker.ByteTrackTracker","title":"<code>trackers.core.bytetrack.tracker.ByteTrackTracker</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Track objects using ByteTrack algorithm with two-stage association. Associates both high and low confidence detections to reduce fragmentation and improve tracking through occlusions.</p> <p>Parameters:</p> Name Type Description Default <code>lost_track_buffer</code> <code>int</code> <p><code>int</code> specifying number of frames to buffer when a track is lost. Increasing this value enhances occlusion handling but may increase ID switching for disappearing objects.</p> <code>30</code> <code>frame_rate</code> <code>float</code> <p><code>float</code> specifying video frame rate in frames per second. Used to scale the lost track buffer for consistent tracking across different frame rates.</p> <code>30.0</code> <code>track_activation_threshold</code> <code>float</code> <p><code>float</code> specifying minimum detection confidence to create new tracks. Higher values reduce false positives but may miss low-confidence objects.</p> <code>0.7</code> <code>minimum_consecutive_frames</code> <code>int</code> <p><code>int</code> specifying number of consecutive frames before a track is considered valid. Before reaching this threshold, tracks are assigned <code>tracker_id</code> of <code>-1</code>.</p> <code>2</code> <code>minimum_iou_threshold</code> <code>float</code> <p><code>float</code> specifying IoU threshold for associating detections to existing tracks. Higher values require more overlap.</p> <code>0.1</code> <code>high_conf_det_threshold</code> <code>float</code> <p><code>float</code> specifying threshold for separating high and low confidence detections in the two-stage association.</p> <code>0.6</code>"},{"location":"api-trackers/#trackers.core.bytetrack.tracker.ByteTrackTracker.update","title":"<code>update(detections)</code>","text":"<p>Update tracker state with new detections and return tracked objects. Performs Kalman filter prediction, two-stage association (high then low confidence), and initializes new tracks for unmatched detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p><code>sv.Detections</code> containing bounding boxes with shape <code>(N, 4)</code> in <code>(x_min, y_min, x_max, y_max)</code> format and optional confidence scores.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p><code>sv.Detections</code> with <code>tracker_id</code> assigned for each detection. Unmatched detections have <code>tracker_id</code> of <code>-1</code>. Detection order may differ from input.</p>"},{"location":"api-trackers/#trackers.core.bytetrack.tracker.ByteTrackTracker.reset","title":"<code>reset()</code>","text":"<p>Reset tracker state by clearing all tracks and resetting ID counter. Call this method when switching to a new video or scene.</p>"},{"location":"learn/evaluate/","title":"Evaluate Trackers","text":"<p>Measure how well your multi-object tracker performs using standard MOT metrics (CLEAR, HOTA, Identity). Get clear, reproducible scores for development, comparison, and publication.</p> <p>What you'll learn:</p> <ul> <li>Evaluate single and multi-sequence tracking results</li> <li>Interpret HOTA, MOTA, and IDF1 scores</li> <li>Prepare datasets in MOT Challenge format</li> </ul>"},{"location":"learn/evaluate/#installation","title":"Installation","text":"<pre><code>pip install trackers\n</code></pre> <p>For alternative methods, see the Install guide.</p>"},{"location":"learn/evaluate/#quickstart","title":"Quickstart","text":"<p>Evaluate a single sequence by pointing to your ground-truth and tracker files, then view the results as a formatted table.</p> PythonCLI <pre><code>from trackers.eval import evaluate_mot_sequence\n\nresult = evaluate_mot_sequence(\n    gt_path=\"data/gt/MOT17-02-FRCNN.txt\",\n    tracker_path=\"data/trackers/MOT17-02-FRCNN.txt\",\n    metrics=[\"CLEAR\", \"HOTA\", \"Identity\"],\n)\n\nprint(result.table(columns=[\"MOTA\", \"HOTA\", \"IDF1\", \"IDSW\"]))\n</code></pre> <p>Output:</p> <pre><code>Sequence                        MOTA    HOTA    IDF1  IDSW\n----------------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100    42\n</code></pre> <pre><code>trackers eval \\\n    --gt data/gt/MOT17-02-FRCNN.txt \\\n    --tracker data/trackers/MOT17-02-FRCNN.txt \\\n    --metrics CLEAR HOTA Identity \\\n    --columns MOTA HOTA IDF1 IDSW\n</code></pre> <p>Output:</p> <pre><code>Sequence                        MOTA    HOTA    IDF1  IDSW\n----------------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100    42\n</code></pre>"},{"location":"learn/evaluate/#data-format","title":"Data Format","text":"<p>Ground truth and tracker files use MOT Challenge text format \u2014 a simple comma-separated .txt file where each line describes one detection.</p> <pre><code>&lt;frame&gt;,&lt;id&gt;,&lt;bb_left&gt;,&lt;bb_top&gt;,&lt;bb_width&gt;,&lt;bb_height&gt;,&lt;conf&gt;,&lt;x&gt;,&lt;y&gt;,&lt;z&gt;\n</code></pre> <p>Example:</p> <pre><code>1,1,100,200,50,80,1,-1,-1,-1\n1,2,300,150,60,90,1,-1,-1,-1\n2,1,105,198,50,80,1,-1,-1,-1\n</code></pre> <p>Fields:</p> <ul> <li><code>frame</code> \u2014 Frame number (1-indexed)</li> <li><code>id</code> \u2014 Unique object ID per track</li> <li><code>bb_left</code>, <code>bb_top</code> \u2014 Top-left bounding box corner</li> <li><code>bb_width</code>, <code>bb_height</code> \u2014 Bounding box dimensions</li> <li><code>conf</code> \u2014 Confidence score (1 for ground truth)</li> <li><code>x</code>, <code>y</code>, <code>z</code> \u2014 3D coordinates (-1 if unused)</li> </ul>"},{"location":"learn/evaluate/#directory-layouts","title":"Directory Layouts","text":"<p>The evaluator automatically detects whether you're using a flat or MOT-style structure. It also tries to infer benchmark name, split, and tracker name from folder names.</p> MOT LayoutFlat Layout <p>Standard MOT Challenge nested structure.</p> <pre><code>data/\n\u251c\u2500\u2500 MOT17-train/\n\u2502   \u251c\u2500\u2500 MOT17-02-FRCNN/\n\u2502   \u2502   \u2514\u2500\u2500 gt/gt.txt\n\u2502   \u251c\u2500\u2500 MOT17-04-FRCNN/\n\u2502   \u2502   \u2514\u2500\u2500 gt/gt.txt\n\u2502   \u2514\u2500\u2500 MOT17-05-FRCNN/\n\u2502       \u2514\u2500\u2500 gt/gt.txt\n\u2514\u2500\u2500 trackers/\n    \u2514\u2500\u2500 MOT17-train/\n        \u2514\u2500\u2500 ByteTrack/\n            \u2514\u2500\u2500 data/\n                \u251c\u2500\u2500 MOT17-02-FRCNN.txt\n                \u251c\u2500\u2500 MOT17-04-FRCNN.txt\n                \u2514\u2500\u2500 MOT17-05-FRCNN.txt\n</code></pre> <p>Python</p> <pre><code>from trackers.eval import evaluate_mot_sequences\n\nresult = evaluate_mot_sequences(\n    gt_dir=\"data\",\n    tracker_dir=\"data/trackers\",\n    benchmark=\"MOT17\",\n    split=\"train\",\n    tracker_name=\"ByteTrack\",\n)\n</code></pre> <p>CLI</p> <pre><code>trackers eval \\\n    --gt-dir data \\\n    --tracker-dir data/trackers \\\n    --benchmark MOT17 \\\n    --split train \\\n    --tracker-name ByteTrack\n</code></pre> <p>One <code>.txt</code> file per sequence, placed directly in the directories.</p> <pre><code>data/\n\u251c\u2500\u2500 gt/\n\u2502   \u251c\u2500\u2500 MOT17-02-FRCNN.txt\n\u2502   \u251c\u2500\u2500 MOT17-04-FRCNN.txt\n\u2502   \u2514\u2500\u2500 MOT17-05-FRCNN.txt\n\u2514\u2500\u2500 trackers/\n    \u251c\u2500\u2500 MOT17-02-FRCNN.txt\n    \u251c\u2500\u2500 MOT17-04-FRCNN.txt\n    \u2514\u2500\u2500 MOT17-05-FRCNN.txt\n</code></pre> <p>Python</p> <pre><code>from trackers.eval import evaluate_mot_sequences\n\nresult = evaluate_mot_sequences(\n    gt_dir=\"data/gt\",\n    tracker_dir=\"data/trackers\",\n)\n</code></pre> <p>CLI</p> <pre><code>trackers eval --gt-dir data/gt --tracker-dir data/trackers\n</code></pre>"},{"location":"learn/evaluate/#multi-sequence-evaluation","title":"Multi-Sequence Evaluation","text":"<p>Run evaluation across many sequences and get both per-sequence results and a combined aggregate.</p> CLIPython <pre><code>trackers eval \\\n    --gt-dir data/gt \\\n    --tracker-dir data/trackers \\\n    --metrics CLEAR HOTA Identity \\\n    --columns MOTA HOTA IDF1 \\\n    --output results.json\n</code></pre> <pre><code>from trackers.eval import evaluate_mot_sequences\n\nresult = evaluate_mot_sequences(\n    gt_dir=\"data/gt\",\n    tracker_dir=\"data/trackers\",\n    metrics=[\"CLEAR\", \"HOTA\", \"Identity\"],\n)\n\nprint(result.table(columns=[\"MOTA\", \"HOTA\", \"IDF1\"]))\n</code></pre> <p>Output:</p> <pre><code>Sequence                        MOTA    HOTA    IDF1\n----------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100\nMOT17-04-FRCNN                78.200  65.100  74.800\nMOT17-05-FRCNN                71.300  59.800  69.200\n----------------------------------------------------\nCOMBINED                      75.033  62.400  72.033\n</code></pre>"},{"location":"learn/install/","title":"Install Trackers","text":"<p>Get up and running with Trackers in minutes. Choose your preferred package manager and start tracking objects in video.</p> <p>What you'll learn:</p> <ul> <li>Install Trackers with <code>pip</code> or <code>uv</code></li> <li>Set up a development environment</li> </ul> <p>Requirements</p> <p>Python <code>3.10</code> or higher is required.</p>"},{"location":"learn/install/#quickstart","title":"Quickstart","text":"pipuvFrom Source <pre><code>pip install trackers\n</code></pre> <pre><code>uv pip install trackers\n</code></pre> <p>For <code>uv</code>-managed projects:</p> <pre><code>uv add trackers\n</code></pre> <p>Install the latest development version:</p> <pre><code>pip install https://github.com/roboflow/trackers/archive/refs/heads/develop.zip\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import trackers; print(trackers.__version__)\"\n</code></pre>"},{"location":"learn/install/#development-setup","title":"Development Setup","text":"<p>Set up a local environment for contributing or modifying Trackers.</p> virtualenvuv <pre><code># Clone and enter repository\ngit clone --depth 1 -b develop https://github.com/roboflow/trackers.git\ncd trackers\n\n# Create and activate environment\npython3.10 -m venv venv\nsource venv/bin/activate\n\n# Install in editable mode\npip install --upgrade pip\npip install -e \".\"\n</code></pre> <pre><code># Clone and enter repository\ngit clone --depth 1 -b develop https://github.com/roboflow/trackers.git\ncd trackers\n\n# Set up environment\nuv python pin 3.10\nuv sync\nuv pip install -e . --all-extras\n</code></pre> <p>Verify dev install:</p> <pre><code>python -c \"import trackers; print(trackers.__version__)\"\n</code></pre>"},{"location":"trackers/bytetrack/","title":"ByteTrack","text":""},{"location":"trackers/bytetrack/#overview","title":"Overview","text":"<p>ByteTrack builds on the same Kalman filter plus Hungarian algorithm framework as SORT but changes the data association strategy to use almost every detection box regardless of confidence score. It runs a two-stage matching: first match high-confidence detections to tracks, then match low-confidence detections to any unmatched tracks using IoU. This reduces missed tracks and fragmentation for occluded or weak detections while retaining simplicity and high frame rates. ByteTrack has set state-of-the-art results on standard MOT benchmarks with real-time performance, because it recovers valid low-score detections instead of discarding them.</p>"},{"location":"trackers/bytetrack/#comparison","title":"Comparison","text":"<p>For comparisons with other trackers, plus dataset context and evaluation details, see the tracker comparison page.</p> Dataset HOTA IDF1 MOTA MOT17 60.1 73.2 74.1 SportsMOT 73.0 72.5 96.4 SoccerNet 84.0 78.1 97.8"},{"location":"trackers/bytetrack/#run-on-video-webcam-or-rtsp-stream","title":"Run on video, webcam, or RTSP stream","text":"<p>These examples use OpenCV for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code>, <code>&lt;WEBCAM_INDEX&gt;</code>, and <code>&lt;RTSP_STREAM_URL&gt;</code> with your inputs. <code>&lt;WEBCAM_INDEX&gt;</code> is usually 0 for the default camera.</p> VideoWebcamRTSP <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;WEBCAM_INDEX&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open webcam\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;RTSP_STREAM_URL&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open RTSP stream\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"trackers/comparison/","title":"Tracker Comparison","text":"<p>This page shows head-to-head performance of SORT and ByteTrack on standard MOT benchmarks.</p>"},{"location":"trackers/comparison/#mot17","title":"MOT17","text":"<p>Pedestrian tracking with crowded scenes and frequent occlusions. Strongly tests re-identification and identity stability.</p> Tracker HOTA IDF1 MOTA SORT 58.4 69.9 67.2 ByteTrack 60.1 73.2 74.1"},{"location":"trackers/comparison/#sportsmot","title":"SportsMOT","text":"<p>Sports broadcast tracking with fast motion, camera pans, and similar-looking targets. Tests association under speed and appearance ambiguity.</p> Tracker HOTA IDF1 MOTA SORT 70.9 68.9 95.7 ByteTrack 73.0 72.5 96.4"},{"location":"trackers/comparison/#soccernet-tracking","title":"SoccerNet-tracking","text":"<p>Long sequences with dense interactions and partial occlusions. Tests long-term ID consistency.</p> Tracker HOTA IDF1 MOTA SORT 81.6 76.2 95.1 ByteTrack 84.0 78.1 97.8"},{"location":"trackers/sort/","title":"SORT","text":""},{"location":"trackers/sort/#overview","title":"Overview","text":"<p>SORT is a classic online, tracking-by-detection method that predicts object motion with a Kalman filter and matches predicted tracks to detections using the Hungarian algorithm based on Intersection over Union (IoU). The tracker uses only geometric cues from bounding boxes, without appearance features, so it runs extremely fast and scales to hundreds of frames per second on typical hardware. Detections from a strong CNN detector feed SORT, which updates each track\u2019s state via a constant velocity motion model and prunes stale tracks. Because SORT lacks explicit re-identification or appearance cues, it can suffer identity switches and fragmented tracks under long occlusions or heavy crowding.</p>"},{"location":"trackers/sort/#comparison","title":"Comparison","text":"<p>For comparisons with other trackers, plus dataset context and evaluation details, see the tracker comparison page.</p> Dataset HOTA IDF1 MOTA MOT17 58.4 69.9 67.2 SportsMOT 70.9 68.9 95.7 SoccerNet 81.6 76.2 95.1"},{"location":"trackers/sort/#run-on-video-webcam-or-rtsp-stream","title":"Run on video, webcam, or RTSP stream","text":"<p>These examples use OpenCV for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code>, <code>&lt;WEBCAM_INDEX&gt;</code>, and <code>&lt;RTSP_STREAM_URL&gt;</code> with your inputs. <code>&lt;WEBCAM_INDEX&gt;</code> is usually 0 for the default camera.</p> VideoWebcamRTSP <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;WEBCAM_INDEX&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open webcam\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;RTSP_STREAM_URL&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open RTSP stream\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"}]}