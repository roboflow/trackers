{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Quickstart","text":"<p><code>trackers</code> gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use.</p>"},{"location":"#install","title":"Install","text":"<p>You can install and use <code>trackers</code> in a Python&gt;=3.10 environment. For detailed installation instructions, including installing from source and setting up a local development environment, check out our install page.</p> <p>Installation</p> <p> </p> pipuv <pre><code>pip install trackers\n</code></pre> <pre><code>uv pip install trackers\n</code></pre>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li> <p>How to Track Objects with SORT</p> <p></p> <p>End-to-end example showing how to run RF-DETR detection with the SORT tracker.</p> <p> Run Google Colab</p> </li> <li> <p>How to Track Objects with ByteTrack</p> <p></p> <p>End-to-end example showing how to run RF-DETR detection with the ByteTrack tracker.</p> <p> Run Google Colab</p> </li> </ul>"},{"location":"#tracking-algorithms","title":"Tracking Algorithms","text":"<p><code>trackers</code> gives you clean, modular re-implementations of leading multi-object tracking algorithms. The package currently supports SORT and ByteTrack. OC-SORT, BoT-SORT, and McByte support is coming soon. For comparisons, see the tracker comparison page.</p> Algorithm MOT17 HOTA MOT17 IDF1 MOT17 MOTA SportsMOT HOTA SoccerNet HOTA SORT 58.4 69.9 67.2 70.9 81.6 ByteTrack 60.1 73.2 74.1 73.0 84.0 OC-SORT \u2014 \u2014 \u2014 \u2014 \u2014 BoT-SORT \u2014 \u2014 \u2014 \u2014 \u2014 McByte \u2014 \u2014 \u2014 \u2014 \u2014"},{"location":"#integration","title":"Integration","text":"<p>With a modular design, <code>trackers</code> lets you combine object detectors from different libraries with the tracker of your choice. See the Track guide for CLI usage and more options. These examples use <code>opencv-python</code> for decoding and display.</p> RF-DETRInferenceUltralytics <pre><code>import cv2\nfrom rfdetr import RFDETRNano\nfrom trackers import ByteTrackTracker\n\nmodel = RFDETRNano()\ntracker = ByteTrackTracker()\n\ncap = cv2.VideoCapture(\"source.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(model_id=\"rfdetr-nano\")\ntracker = ByteTrackTracker()\n\ncap = cv2.VideoCapture(\"source.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\nfrom trackers import ByteTrackTracker\n\nmodel = YOLO(\"yolo11n.pt\")\ntracker = ByteTrackTracker()\n\ncap = cv2.VideoCapture(\"source.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(result)\n    detections = tracker.update(detections)\n</code></pre>"},{"location":"api/evals/","title":"Evals API","text":""},{"location":"api/evals/#trackers.eval.evaluate.evaluate_mot_sequence","title":"<code>trackers.eval.evaluate.evaluate_mot_sequence(gt_path, tracker_path, metrics=None, threshold=0.5)</code>","text":"<p>Evaluate a single multi-object tracking result against ground truth. Computes standard multi-object tracking metrics (CLEAR MOT, HOTA, Identity) for one sequence by matching predicted tracks to ground-truth tracks using per-frame IoU (Intersection over Union).</p> <p>TrackEval parity</p> <p>This evaluation code is intentionally designed to match the core matching logic and metric calculations of TrackEval.</p> <p>Parameters:</p> Name Type Description Default <code>gt_path</code> <code>str | Path</code> <p>Path to the ground-truth MOT file.</p> required <code>tracker_path</code> <code>str | Path</code> <p>Path to the tracker MOT file.</p> required <code>metrics</code> <code>list[str] | None</code> <p>Metric families to compute. Supported values are <code>[\"CLEAR\", \"HOTA\", \"Identity\"]</code>. Defaults to <code>[\"CLEAR\"]</code>.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>IoU threshold for <code>CLEAR</code> and <code>Identity</code> matching. Defaults to <code>0.5</code>. <code>HOTA</code> evaluates across multiple thresholds internally.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>SequenceResult</code> <p><code>SequenceResult</code> with <code>CLEAR</code>, <code>HOTA</code>, and/or <code>Identity</code> populated based on <code>metrics</code>.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>gt_path</code> or <code>tracker_path</code> does not exist.</p> <code>ValueError</code> <p>If an unsupported metric family is requested.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from trackers.eval import evaluate_mot_sequence\n&gt;&gt;&gt;\n&gt;&gt;&gt; result = evaluate_mot_sequence(\n...     gt_path=\"data/gt/MOT17-02/gt.txt\",\n...     tracker_path=\"data/trackers/MOT17-02.txt\",\n...     metrics=[\"CLEAR\", \"HOTA\", \"Identity\"],\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; result.CLEAR.MOTA\n0.756\n&gt;&gt;&gt;\n&gt;&gt;&gt; result.table(columns=[\"MOTA\", \"HOTA\", \"IDF1\", \"IDSW\"])\nSequence    MOTA    HOTA    IDF1  IDSW\n--------------------------------------\nMOT17-02  75.600  62.300  72.100    42\n</code></pre>"},{"location":"api/evals/#trackers.eval.evaluate.evaluate_mot_sequences","title":"<code>trackers.eval.evaluate.evaluate_mot_sequences(gt_dir, tracker_dir, seqmap=None, metrics=None, threshold=0.5)</code>","text":"<p>Evaluate multiple multi-object tracking results against ground truth. Computes standard multi-object tracking metrics (CLEAR MOT, HOTA, Identity) across one or more sequences by matching predicted tracks to ground-truth tracks using per-frame IoU (Intersection over Union). Returns both per-sequence and aggregated (combined) results.</p> <p>TrackEval parity</p> <p>This evaluation code is intentionally designed to match the core matching logic and metric calculations of TrackEval.</p> <p>Supported dataset layouts</p> <p>Both <code>gt_dir</code> and <code>tracker_dir</code> should point directly at the parent directory of the sequences. The evaluator auto-detects which layout you're using.</p> MOT layoutFlat layout <pre><code>gt_dir/\n\u251c\u2500\u2500 MOT17-02-FRCNN/\n\u2502   \u2514\u2500\u2500 gt/gt.txt\n\u251c\u2500\u2500 MOT17-04-FRCNN/\n\u2502   \u2514\u2500\u2500 gt/gt.txt\n\u2514\u2500\u2500 MOT17-05-FRCNN/\n    \u2514\u2500\u2500 gt/gt.txt\n\ntracker_dir/\n\u251c\u2500\u2500 MOT17-02-FRCNN.txt\n\u251c\u2500\u2500 MOT17-04-FRCNN.txt\n\u2514\u2500\u2500 MOT17-05-FRCNN.txt\n</code></pre> <pre><code>gt_dir/\n\u251c\u2500\u2500 MOT17-02.txt\n\u251c\u2500\u2500 MOT17-04.txt\n\u251c\u2500\u2500 MOT17-05.txt\n\u2514\u2500\u2500 ...\n\ntracker_dir/\n\u251c\u2500\u2500 MOT17-02.txt\n\u251c\u2500\u2500 MOT17-04.txt\n\u251c\u2500\u2500 MOT17-05.txt\n\u2514\u2500\u2500 ...\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>gt_dir</code> <code>str | Path</code> <p>Directory containing ground-truth data. Should be the direct parent of the sequence folders (MOT layout) or sequence files (flat layout).</p> required <code>tracker_dir</code> <code>str | Path</code> <p>Directory containing tracker prediction files (<code>{seq}.txt</code>).</p> required <code>seqmap</code> <code>str | Path | None</code> <p>Optional sequence map. If provided, only those sequences are evaluated.</p> <code>None</code> <code>metrics</code> <code>list[str] | None</code> <p>Metric families to compute. Supported values are <code>[\"CLEAR\", \"HOTA\", \"Identity\"]</code>. Defaults to <code>[\"CLEAR\"]</code>.</p> <code>None</code> <code>threshold</code> <code>float</code> <p>IoU threshold for <code>CLEAR</code> and <code>Identity</code>. Defaults to <code>0.5</code>.</p> <code>0.5</code> <p>Returns:</p> Type Description <code>BenchmarkResult</code> <p><code>BenchmarkResult</code> with per-sequence results and a <code>COMBINED</code> aggregate.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If <code>gt_dir</code> or <code>tracker_dir</code> does not exist.</p> <code>ValueError</code> <p>If no sequences are found.</p> <p>Examples:</p> <p>Auto-detect layout and evaluate all sequences:</p> <pre><code>&gt;&gt;&gt; from trackers.eval import evaluate_mot_sequences\n&gt;&gt;&gt;\n&gt;&gt;&gt; result = evaluate_mot_sequences(\n...     gt_dir=\"data/gt/\",\n...     tracker_dir=\"data/trackers/\",\n...     metrics=[\"CLEAR\", \"HOTA\", \"Identity\"],\n... )\n&gt;&gt;&gt;\n&gt;&gt;&gt; result.table(columns=[\"MOTA\", \"HOTA\", \"IDF1\", \"IDSW\"])\nSequence     MOTA    HOTA    IDF1  IDSW\n---------------------------------------\nsequence1  74.800  60.900  71.200    37\nsequence2  76.100  63.200  72.500    45\n---------------------------------------\nCOMBINED   75.450  62.050  71.850    82\n</code></pre>"},{"location":"api/evals/#trackers.eval.results.SequenceResult","title":"<code>trackers.eval.results.SequenceResult</code>  <code>dataclass</code>","text":"<p>Result for a single sequence evaluation.</p> <p>Attributes:</p> Name Type Description <code>sequence</code> <code>str</code> <p>Name of the sequence.</p> <code>CLEAR</code> <code>CLEARMetrics | None</code> <p>CLEAR metrics for this sequence, or <code>None</code> if not requested.</p> <code>HOTA</code> <code>HOTAMetrics | None</code> <p>HOTA metrics for this sequence, or <code>None</code> if not requested.</p> <code>Identity</code> <code>IdentityMetrics | None</code> <p>Identity metrics for this sequence, or <code>None</code> if not requested.</p>"},{"location":"api/evals/#trackers.eval.results.SequenceResult.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>SequenceResult</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with sequence name and metrics.</p> required <p>Returns:</p> Type Description <code>SequenceResult</code> <p><code>SequenceResult</code> instance.</p>"},{"location":"api/evals/#trackers.eval.results.SequenceResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api/evals/#trackers.eval.results.SequenceResult.json","title":"<code>json(indent=2)</code>","text":"<p>Serialize to JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int</code> <p>Indentation level for formatting. Defaults to <code>2</code>.</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation.</p>"},{"location":"api/evals/#trackers.eval.results.SequenceResult.table","title":"<code>table(columns=None)</code>","text":"<p>Format as a table string.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str] | None</code> <p>Metric columns to include. If <code>None</code>, includes all available metrics.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted table string.</p>"},{"location":"api/evals/#trackers.eval.results.BenchmarkResult","title":"<code>trackers.eval.results.BenchmarkResult</code>  <code>dataclass</code>","text":"<p>Result for multi-sequence evaluation.</p> <p>Attributes:</p> Name Type Description <code>sequences</code> <code>dict[str, SequenceResult]</code> <p>Dictionary mapping sequence names to their results.</p> <code>aggregate</code> <code>SequenceResult</code> <p>Combined metrics across all sequences.</p>"},{"location":"api/evals/#trackers.eval.results.BenchmarkResult.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>BenchmarkResult</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with sequences and aggregate results.</p> required <p>Returns:</p> Type Description <code>BenchmarkResult</code> <p><code>BenchmarkResult</code> instance.</p>"},{"location":"api/evals/#trackers.eval.results.BenchmarkResult.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api/evals/#trackers.eval.results.BenchmarkResult.json","title":"<code>json(indent=2)</code>","text":"<p>Serialize to JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>indent</code> <code>int</code> <p>Indentation level for formatting. Defaults to <code>2</code>.</p> <code>2</code> <p>Returns:</p> Type Description <code>str</code> <p>JSON string representation.</p>"},{"location":"api/evals/#trackers.eval.results.BenchmarkResult.table","title":"<code>table(columns=None)</code>","text":"<p>Format as a table string.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str] | None</code> <p>Metric columns to include. If <code>None</code>, includes all available metrics.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Formatted table string.</p>"},{"location":"api/evals/#trackers.eval.results.BenchmarkResult.save","title":"<code>save(path)</code>","text":"<p>Save to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Destination file path.</p> required"},{"location":"api/evals/#trackers.eval.results.BenchmarkResult.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load from a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Source file path.</p> required <p>Returns:</p> Type Description <code>BenchmarkResult</code> <p><code>BenchmarkResult</code> instance.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file does not exist.</p>"},{"location":"api/evals/#trackers.eval.results.CLEARMetrics","title":"<code>trackers.eval.results.CLEARMetrics</code>  <code>dataclass</code>","text":"<p>CLEAR metrics with TrackEval-compatible field names. Float metrics are stored as fractions (0-1 range), not percentages. The values follow the original CLEAR MOT definitions.</p> <p>Attributes:</p> Name Type Description <code>MOTA</code> <code>float</code> <p>Multiple Object Tracking Accuracy. Penalizes false negatives, false positives, and ID switches: <code>(TP - FP - IDSW) / (TP + FN)</code>. Can be negative when errors exceed matches.</p> <code>MOTP</code> <code>float</code> <p>Multiple Object Tracking Precision. Mean IoU of matched pairs. Measures localization quality only.</p> <code>MODA</code> <code>float</code> <p>Multiple Object Detection Accuracy. Like MOTA but ignores ID switches: <code>(TP - FP) / (TP + FN)</code>.</p> <code>CLR_Re</code> <code>float</code> <p>CLEAR recall. Fraction of GT detections matched: <code>TP / (TP + FN)</code>.</p> <code>CLR_Pr</code> <code>float</code> <p>CLEAR precision. Fraction of tracker detections correct: <code>TP / (TP + FP)</code>.</p> <code>MTR</code> <code>float</code> <p>Mostly tracked ratio. Fraction of GT tracks tracked for &gt;80% of their lifespan.</p> <code>PTR</code> <code>float</code> <p>Partially tracked ratio. Fraction of GT tracks tracked for 20-80%.</p> <code>MLR</code> <code>float</code> <p>Mostly lost ratio. Fraction of GT tracks tracked for &lt;20%.</p> <code>sMOTA</code> <code>float</code> <p>Summed MOTA. Replaces TP count with IoU sum: <code>(MOTP_sum - FP - IDSW) / (TP + FN)</code>.</p> <code>CLR_TP</code> <code>int</code> <p>True positives. Number of correct matches.</p> <code>CLR_FN</code> <code>int</code> <p>False negatives. Number of missed GT detections.</p> <code>CLR_FP</code> <code>int</code> <p>False positives. Number of spurious tracker detections.</p> <code>IDSW</code> <code>int</code> <p>ID switches. Times a GT track changes its matched tracker ID.</p> <code>MT</code> <code>int</code> <p>Mostly tracked count. Number of GT tracks tracked &gt;80%.</p> <code>PT</code> <code>int</code> <p>Partially tracked count. Number of GT tracks tracked 20-80%.</p> <code>ML</code> <code>int</code> <p>Mostly lost count. Number of GT tracks tracked &lt;20%.</p> <code>Frag</code> <code>int</code> <p>Fragmentations. Times a tracked GT becomes untracked then tracked again.</p> <code>MOTP_sum</code> <code>float</code> <p>Raw IoU sum for aggregation across sequences.</p> <code>CLR_Frames</code> <code>int</code> <p>Number of frames evaluated.</p>"},{"location":"api/evals/#trackers.eval.results.CLEARMetrics.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>CLEARMetrics</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with metric values.</p> required <p>Returns:</p> Type Description <code>CLEARMetrics</code> <p><code>CLEARMetrics</code> instance.</p>"},{"location":"api/evals/#trackers.eval.results.CLEARMetrics.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api/evals/#trackers.eval.results.HOTAMetrics","title":"<code>trackers.eval.results.HOTAMetrics</code>  <code>dataclass</code>","text":"<p>HOTA metrics with TrackEval-compatible field names. HOTA evaluates both detection quality and association quality. Float metrics are stored as fractions (0-1 range).</p> <p>Attributes:</p> Name Type Description <code>HOTA</code> <code>float</code> <p>Higher Order Tracking Accuracy. Geometric mean of DetA and AssA, averaged over 19 IoU thresholds (0.05 to 0.95).</p> <code>DetA</code> <code>float</code> <p>Detection accuracy: <code>TP / (TP + FN + FP)</code>.</p> <code>AssA</code> <code>float</code> <p>Association accuracy for matched detections over time.</p> <code>DetRe</code> <code>float</code> <p>Detection recall: <code>TP / (TP + FN)</code>.</p> <code>DetPr</code> <code>float</code> <p>Detection precision: <code>TP / (TP + FP)</code>.</p> <code>AssRe</code> <code>float</code> <p>Association recall. For each GT ID, measures how consistently it maps to a single tracker ID across time.</p> <code>AssPr</code> <code>float</code> <p>Association precision. For each tracker ID, measures how consistently it maps to a single GT ID across time.</p> <code>LocA</code> <code>float</code> <p>Localization accuracy. Mean IoU for matched pairs.</p> <code>OWTA</code> <code>float</code> <p>Open World Tracking Accuracy. <code>sqrt(DetRe * AssA)</code>, useful when precision is less meaningful.</p> <code>HOTA_TP</code> <code>int</code> <p>True positive count summed over all 19 thresholds.</p> <code>HOTA_FN</code> <code>int</code> <p>False negative count summed over all 19 thresholds.</p> <code>HOTA_FP</code> <code>int</code> <p>False positive count summed over all 19 thresholds.</p>"},{"location":"api/evals/#trackers.eval.results.HOTAMetrics.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>HOTAMetrics</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with metric values.</p> required <p>Returns:</p> Type Description <code>HOTAMetrics</code> <p><code>HOTAMetrics</code> instance.</p>"},{"location":"api/evals/#trackers.eval.results.HOTAMetrics.to_dict","title":"<code>to_dict(include_arrays=False, arrays_as_list=True)</code>","text":"<p>Convert to dictionary representation.</p> <p>Parameters:</p> Name Type Description Default <code>include_arrays</code> <code>bool</code> <p>Whether to include per-alpha arrays. Defaults to <code>False</code>.</p> <code>False</code> <code>arrays_as_list</code> <code>bool</code> <p>Whether to convert arrays to lists for JSON serialization. Defaults to <code>True</code>. Set to <code>False</code> to keep numpy arrays.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api/evals/#trackers.eval.results.IdentityMetrics","title":"<code>trackers.eval.results.IdentityMetrics</code>  <code>dataclass</code>","text":"<p>Identity metrics with TrackEval-compatible field names. Identity metrics measure global ID consistency using an optimal one-to-one assignment between GT and tracker IDs across the full sequence.</p> <p>Attributes:</p> Name Type Description <code>IDF1</code> <code>float</code> <p>ID F1 score. Harmonic mean of IDR and IDP, the primary identity metric.</p> <code>IDR</code> <code>float</code> <p>ID recall. <code>IDTP / (IDTP + IDFN)</code>, fraction of GT detections with correct global ID assignment.</p> <code>IDP</code> <code>float</code> <p>ID precision. <code>IDTP / (IDTP + IDFP)</code>, fraction of tracker detections with correct global ID assignment.</p> <code>IDTP</code> <code>int</code> <p>ID true positives. Detections matched with globally consistent IDs.</p> <code>IDFN</code> <code>int</code> <p>ID false negatives. GT detections not matched or matched to the wrong global ID.</p> <code>IDFP</code> <code>int</code> <p>ID false positives. Tracker detections not matched or matched to the wrong global ID.</p>"},{"location":"api/evals/#trackers.eval.results.IdentityMetrics.from_dict","title":"<code>from_dict(data)</code>  <code>classmethod</code>","text":"<p>Create <code>IdentityMetrics</code> from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>dict[str, Any]</code> <p>Dictionary with metric values.</p> required <p>Returns:</p> Type Description <code>IdentityMetrics</code> <p><code>IdentityMetrics</code> instance.</p>"},{"location":"api/evals/#trackers.eval.results.IdentityMetrics.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert to dictionary representation.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary with all metric values.</p>"},{"location":"api/io/","title":"I/O API","text":""},{"location":"api/io/#trackers.io.video.frames_from_source","title":"<code>trackers.io.video.frames_from_source(source)</code>","text":"<p>Yield numbered BGR frames from video files, webcams, network streams, or image directories.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | Path | int</code> <p>Video file path, RTSP/HTTP stream URL, webcam index, or path to a directory containing images (<code>.jpg</code>, <code>.jpeg</code>, <code>.png</code>, <code>.bmp</code>, <code>.tif</code>, <code>.tiff</code>).</p> required <p>Returns:</p> Type Description <code>Iterator[tuple[int, ndarray]]</code> <p>Iterator of <code>(frame_id, frame)</code> tuples where <code>frame_id</code> is 1-based and <code>frame</code></p> <code>Iterator[tuple[int, ndarray]]</code> <p>is a <code>np.ndarray</code> in BGR format.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Source cannot be opened or directory contains no supported images.</p> <code>OSError</code> <p>Image file exists but cannot be decoded / read.</p> <code>RuntimeError</code> <p>Capture read failure after successful open.</p>"},{"location":"api/motion/","title":"Motion API","text":""},{"location":"api/motion/#motionestimator","title":"MotionEstimator","text":""},{"location":"api/motion/#trackers.motion.estimator.MotionEstimator","title":"<code>trackers.motion.estimator.MotionEstimator</code>","text":"<p>Estimates camera motion between consecutive video frames.</p> <p>Uses sparse optical flow (Lucas-Kanade) to track feature points and computes the geometric transformation between frames. Accumulates transformations to maintain a consistent world coordinate system relative to the first frame.</p> <p>Parameters:</p> Name Type Description Default <code>max_points</code> <code>int</code> <p>Maximum number of feature points to track. More points increase accuracy but reduce speed. Default: 200.</p> <code>200</code> <code>min_distance</code> <code>int</code> <p>Minimum distance between detected feature points. Larger values spread points more evenly. Default: 15.</p> <code>15</code> <code>block_size</code> <code>int</code> <p>Size of the averaging block for corner detection. Larger values make detection less sensitive to noise. Default: 3.</p> <code>3</code> <code>quality_level</code> <code>float</code> <p>Quality threshold for corner detection (0-1). Higher values detect fewer, stronger corners. Default: 0.01.</p> <code>0.01</code> <code>ransac_reproj_threshold</code> <code>float</code> <p>RANSAC inlier threshold in pixels. Points with reprojection error below this are considered inliers. Default: 3.0.</p> <code>3.0</code> Example <pre><code>import cv2\nfrom trackers.motion import MotionEstimator\n\nestimator = MotionEstimator()\n\ncap = cv2.VideoCapture(\"video.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Get transformation for this frame\n    coord_transform = estimator.update(frame)\n\n    # Transform trajectory points from world to frame coordinates\n    frame_points = coord_transform.abs_to_rel(world_trajectory)\n</code></pre>"},{"location":"api/motion/#trackers.motion.estimator.MotionEstimator.update","title":"<code>update(frame)</code>","text":"<p>Process a new frame and return the coordinate transformation.</p> <p>The returned transformation converts between: - Absolute coordinates: Position relative to the first frame - Relative coordinates: Position in the current frame</p> <p>Parameters:</p> Name Type Description Default <code>frame</code> <code>ndarray</code> <p>Current video frame (BGR or grayscale).</p> required <p>Returns:</p> Type Description <code>CoordinatesTransformation</code> <p><code>CoordinatesTransformation</code> for converting between absolute and relative coordinates. Returns <code>IdentityTransformation</code> for the first frame or if motion estimation fails.</p>"},{"location":"api/motion/#trackers.motion.estimator.MotionEstimator.reset","title":"<code>reset()</code>","text":"<p>Reset the estimator state.</p> <p>Call this when starting a new video or when you want to reset the world coordinate system to the current frame.</p>"},{"location":"api/motion/#motionawaretraceannotator","title":"MotionAwareTraceAnnotator","text":""},{"location":"api/motion/#trackers.annotators.trace.MotionAwareTraceAnnotator","title":"<code>trackers.annotators.trace.MotionAwareTraceAnnotator</code>","text":"<p>Draws object trajectories with camera motion compensation.</p> <p>This annotator maintains a history of object positions in world coordinates and draws them as trajectories (traces) on each frame. When used with camera motion compensation, trajectories appear stable even when the camera moves.</p> <p>The API is compatible with supervision annotators, using the same color resolution strategy and position anchoring.</p> <p>Parameters:</p> Name Type Description Default <code>color</code> <code>Color | ColorPalette | None</code> <p>The color to draw the trace. Can be a single <code>Color</code> or a <code>ColorPalette</code>. Defaults to <code>ColorPalette.DEFAULT</code>.</p> <code>None</code> <code>position</code> <code>Position | None</code> <p>The anchor position on the bounding box for the trace point. Defaults to <code>Position.CENTER</code>.</p> <code>None</code> <code>trace_length</code> <code>int</code> <p>Maximum number of points to store per trajectory. Defaults to <code>30</code>.</p> <code>30</code> <code>thickness</code> <code>int</code> <p>Line thickness for drawing traces. Defaults to <code>2</code>.</p> <code>2</code> <code>color_lookup</code> <code>ColorLookup | None</code> <p>Strategy for mapping colors to annotations. Options are <code>INDEX</code>, <code>CLASS</code>, <code>TRACK</code>. Defaults to <code>ColorLookup.TRACK</code>.</p> <code>None</code> Example <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\n\nfrom trackers import (\n    ByteTrackTracker,\n    MotionAwareTraceAnnotator,\n    MotionEstimator,\n)\n\nmodel = get_model(\"rfdetr-nano\")\ntracker = ByteTrackTracker()\nmotion_estimator = MotionEstimator()\ntrace_annotator = MotionAwareTraceAnnotator()\n\ncap = cv2.VideoCapture(\"moving_camera.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    coord_transform = motion_estimator.update(frame)\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n\n    frame = trace_annotator.annotate(\n        scene=frame,\n        detections=detections,\n        coord_transform=coord_transform,\n    )\n</code></pre>"},{"location":"api/motion/#trackers.annotators.trace.MotionAwareTraceAnnotator.annotate","title":"<code>annotate(scene, detections, custom_color_lookup=None, coord_transform=None)</code>","text":"<p>Draw motion-compensated trace paths on the scene.</p> <p>Updates internal trajectory storage with new detection positions (converted to world coordinates), then draws all trajectories transformed back to frame coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ndarray</code> <p>The image on which traces will be drawn. Modified in place.</p> required <code>detections</code> <code>Detections</code> <p>Detections with <code>tracker_id</code> field populated.</p> required <code>custom_color_lookup</code> <code>ndarray | None</code> <p>Optional custom color lookup array to override the default color mapping strategy.</p> <code>None</code> <code>coord_transform</code> <code>CoordinatesTransformation | None</code> <p>Coordinate transformation for the current frame. If None, uses identity transformation (no motion compensation).</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The annotated image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If detections don't have tracker_id field.</p>"},{"location":"api/motion/#trackers.annotators.trace.MotionAwareTraceAnnotator.reset","title":"<code>reset()</code>","text":"<p>Clear all stored trajectories.</p> <p>Call this when switching videos or when you want to reset trajectory history.</p>"},{"location":"api/motion/#trackers.annotators.trace.MotionAwareTraceAnnotator.clear_tracker","title":"<code>clear_tracker(tracker_id)</code>","text":"<p>Clear the trajectory for a specific tracker ID.</p> <p>Parameters:</p> Name Type Description Default <code>tracker_id</code> <code>int</code> <p>The tracker ID to clear.</p> required"},{"location":"api/motion/#coordinatestransformation","title":"CoordinatesTransformation","text":""},{"location":"api/motion/#trackers.motion.transformation.CoordinatesTransformation","title":"<code>trackers.motion.transformation.CoordinatesTransformation</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for coordinate transformations.</p> <p>Subclasses implement specific transformation types that convert points between absolute (world) and relative (frame) coordinates.</p>"},{"location":"api/motion/#trackers.motion.transformation.CoordinatesTransformation.abs_to_rel","title":"<code>abs_to_rel(points)</code>  <code>abstractmethod</code>","text":"<p>Transform points from absolute (world) to relative (frame) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>Array of shape <code>(N, 2)</code> containing <code>(x, y)</code> coordinates in absolute/world space.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape <code>(N, 2)</code> containing <code>(x, y)</code> coordinates</p> <code>ndarray</code> <p>in relative/frame space.</p>"},{"location":"api/motion/#trackers.motion.transformation.CoordinatesTransformation.rel_to_abs","title":"<code>rel_to_abs(points)</code>  <code>abstractmethod</code>","text":"<p>Transform points from relative (frame) to absolute (world) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>ndarray</code> <p>Array of shape <code>(N, 2)</code> containing <code>(x, y)</code> coordinates in relative/frame space.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape <code>(N, 2)</code> containing <code>(x, y)</code> coordinates</p> <code>ndarray</code> <p>in absolute/world space.</p>"},{"location":"api/motion/#identitytransformation","title":"IdentityTransformation","text":""},{"location":"api/motion/#trackers.motion.transformation.IdentityTransformation","title":"<code>trackers.motion.transformation.IdentityTransformation</code>","text":"<p>               Bases: <code>CoordinatesTransformation</code></p> <p>No-op transformation where absolute and relative coordinates are identical.</p> <p>Used for the first frame (before any camera motion is detected) or when motion estimation fails.</p>"},{"location":"api/motion/#trackers.motion.transformation.IdentityTransformation.abs_to_rel","title":"<code>abs_to_rel(points)</code>","text":"<p>Return points unchanged.</p>"},{"location":"api/motion/#trackers.motion.transformation.IdentityTransformation.rel_to_abs","title":"<code>rel_to_abs(points)</code>","text":"<p>Return points unchanged.</p>"},{"location":"api/motion/#homographytransformation","title":"HomographyTransformation","text":""},{"location":"api/motion/#trackers.motion.transformation.HomographyTransformation","title":"<code>trackers.motion.transformation.HomographyTransformation</code>","text":"<p>               Bases: <code>CoordinatesTransformation</code></p> <p>Full perspective transformation using a 3x3 homography matrix.</p> <p>Supports rotation, translation, scaling, and perspective changes. This is the most general transformation type, suitable for any camera motion.</p> <p>The homography matrix maps points from the first frame (absolute) to the current frame (relative). The inverse matrix is computed automatically for the reverse transformation.</p> <p>Parameters:</p> Name Type Description Default <code>homography_matrix</code> <code>ndarray</code> <p>3x3 homography matrix that transforms points from absolute (first frame) coordinates to relative (current frame) coordinates.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the matrix is not 3x3.</p> Example <pre><code>import numpy as np\n\nfrom trackers import HomographyTransformation\n\nhomography_matrix = np.array([\n    [1.0, 0.0, 10.0],\n    [0.0, 1.0, 20.0],\n    [0.0, 0.0, 1.0],\n])\ntransform = HomographyTransformation(homography_matrix)\n\nworld_points = np.array([[100, 200], [300, 400]])\nframe_points = transform.abs_to_rel(world_points)\n</code></pre>"},{"location":"api/motion/#trackers.motion.transformation.HomographyTransformation.abs_to_rel","title":"<code>abs_to_rel(points)</code>","text":"<p>Transform from absolute (world) to relative (frame) coordinates.</p>"},{"location":"api/motion/#trackers.motion.transformation.HomographyTransformation.rel_to_abs","title":"<code>rel_to_abs(points)</code>","text":"<p>Transform from relative (frame) to absolute (world) coordinates.</p>"},{"location":"api/trackers/","title":"Trackers API","text":""},{"location":"api/trackers/#sort","title":"SORT","text":""},{"location":"api/trackers/#trackers.core.sort.tracker.SORTTracker","title":"<code>trackers.core.sort.tracker.SORTTracker</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Track objects using SORT algorithm with Kalman filter and IoU matching. Provides simple and fast online tracking using only bounding box geometry without appearance features.</p> <p>Parameters:</p> Name Type Description Default <code>lost_track_buffer</code> <code>int</code> <p><code>int</code> specifying number of frames to buffer when a track is lost. Increasing this value enhances occlusion handling but may increase ID switching for similar objects.</p> <code>30</code> <code>frame_rate</code> <code>float</code> <p><code>float</code> specifying video frame rate in frames per second. Used to scale the lost track buffer for consistent tracking across different frame rates.</p> <code>30.0</code> <code>track_activation_threshold</code> <code>float</code> <p><code>float</code> specifying minimum detection confidence to create new tracks. Higher values reduce false positives but may miss low-confidence objects.</p> <code>0.25</code> <code>minimum_consecutive_frames</code> <code>int</code> <p><code>int</code> specifying number of consecutive frames before a track is considered valid. Before reaching this threshold, tracks are assigned <code>tracker_id</code> of <code>-1</code>.</p> <code>3</code> <code>minimum_iou_threshold</code> <code>float</code> <p><code>float</code> specifying IoU threshold for associating detections to existing tracks. Higher values require more overlap.</p> <code>0.3</code>"},{"location":"api/trackers/#trackers.core.sort.tracker.SORTTracker.update","title":"<code>update(detections)</code>","text":"<p>Update tracker state with new detections and return tracked objects. Performs Kalman filter prediction, IoU-based association, and initializes new tracks for unmatched high-confidence detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p><code>sv.Detections</code> containing bounding boxes with shape <code>(N, 4)</code> in <code>(x_min, y_min, x_max, y_max)</code> format and optional confidence scores.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p><code>sv.Detections</code> with <code>tracker_id</code> assigned for each detection. Unmatched or immature tracks have <code>tracker_id</code> of <code>-1</code>.</p>"},{"location":"api/trackers/#trackers.core.sort.tracker.SORTTracker.reset","title":"<code>reset()</code>","text":"<p>Reset tracker state by clearing all tracks and resetting ID counter. Call this method when switching to a new video or scene.</p>"},{"location":"api/trackers/#bytetrack","title":"ByteTrack","text":""},{"location":"api/trackers/#trackers.core.bytetrack.tracker.ByteTrackTracker","title":"<code>trackers.core.bytetrack.tracker.ByteTrackTracker</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Track objects using ByteTrack algorithm with two-stage association. Associates both high and low confidence detections to reduce fragmentation and improve tracking through occlusions.</p> <p>Parameters:</p> Name Type Description Default <code>lost_track_buffer</code> <code>int</code> <p><code>int</code> specifying number of frames to buffer when a track is lost. Increasing this value enhances occlusion handling but may increase ID switching for disappearing objects.</p> <code>30</code> <code>frame_rate</code> <code>float</code> <p><code>float</code> specifying video frame rate in frames per second. Used to scale the lost track buffer for consistent tracking across different frame rates.</p> <code>30.0</code> <code>track_activation_threshold</code> <code>float</code> <p><code>float</code> specifying minimum detection confidence to create new tracks. Higher values reduce false positives but may miss low-confidence objects.</p> <code>0.7</code> <code>minimum_consecutive_frames</code> <code>int</code> <p><code>int</code> specifying number of consecutive frames before a track is considered valid. Before reaching this threshold, tracks are assigned <code>tracker_id</code> of <code>-1</code>.</p> <code>2</code> <code>minimum_iou_threshold</code> <code>float</code> <p><code>float</code> specifying IoU threshold for associating detections to existing tracks. Higher values require more overlap.</p> <code>0.1</code> <code>high_conf_det_threshold</code> <code>float</code> <p><code>float</code> specifying threshold for separating high and low confidence detections in the two-stage association.</p> <code>0.6</code>"},{"location":"api/trackers/#trackers.core.bytetrack.tracker.ByteTrackTracker.update","title":"<code>update(detections)</code>","text":"<p>Update tracker state with new detections and return tracked objects. Performs Kalman filter prediction, two-stage association (high then low confidence), and initializes new tracks for unmatched detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p><code>sv.Detections</code> containing bounding boxes with shape <code>(N, 4)</code> in <code>(x_min, y_min, x_max, y_max)</code> format and optional confidence scores.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p><code>sv.Detections</code> with <code>tracker_id</code> assigned for each detection. Unmatched detections have <code>tracker_id</code> of <code>-1</code>. Detection order may differ from input.</p>"},{"location":"api/trackers/#trackers.core.bytetrack.tracker.ByteTrackTracker.reset","title":"<code>reset()</code>","text":"<p>Reset tracker state by clearing all tracks and resetting ID counter. Call this method when switching to a new video or scene.</p>"},{"location":"learn/evaluate/","title":"Evaluate Trackers","text":"<p>Measure how well your multi-object tracker performs using standard MOT metrics (CLEAR, HOTA, Identity). Get clear, reproducible scores for development, comparison, and publication.</p> <p>What you'll learn:</p> <ul> <li>Evaluate single and multi-sequence tracking results</li> <li>Interpret HOTA, MOTA, and IDF1 scores</li> <li>Prepare datasets in MOT Challenge format</li> </ul>"},{"location":"learn/evaluate/#installation","title":"Installation","text":"<pre><code>pip install trackers\n</code></pre> <p>For alternative methods, see the Install guide.</p>"},{"location":"learn/evaluate/#quickstart","title":"Quickstart","text":"<p>Evaluate a single sequence by pointing to your ground-truth and tracker files, then view the results as a formatted table.</p> PythonCLI <pre><code>from trackers.eval import evaluate_mot_sequence\n\nresult = evaluate_mot_sequence(\n    gt_path=\"data/gt/MOT17-02-FRCNN.txt\",\n    tracker_path=\"data/trackers/MOT17-02-FRCNN.txt\",\n    metrics=[\"CLEAR\", \"HOTA\", \"Identity\"],\n)\n\nprint(result.table(columns=[\"MOTA\", \"HOTA\", \"IDF1\", \"IDSW\"]))\n</code></pre> <p>Output:</p> <pre><code>Sequence                        MOTA    HOTA    IDF1  IDSW\n----------------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100    42\n</code></pre> <pre><code>trackers eval \\\n    --gt data/gt/MOT17-02-FRCNN.txt \\\n    --tracker data/trackers/MOT17-02-FRCNN.txt \\\n    --metrics CLEAR HOTA Identity \\\n    --columns MOTA HOTA IDF1 IDSW\n</code></pre> <p>Output:</p> <pre><code>Sequence                        MOTA    HOTA    IDF1  IDSW\n----------------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100    42\n</code></pre>"},{"location":"learn/evaluate/#data-format","title":"Data Format","text":"<p>Ground truth and tracker files use MOT Challenge text format \u2014 a simple comma-separated .txt file where each line describes one detection.</p> <pre><code>&lt;frame&gt;,&lt;id&gt;,&lt;bb_left&gt;,&lt;bb_top&gt;,&lt;bb_width&gt;,&lt;bb_height&gt;,&lt;conf&gt;,&lt;x&gt;,&lt;y&gt;,&lt;z&gt;\n</code></pre> <p>Example:</p> <pre><code>1,1,100,200,50,80,1,-1,-1,-1\n1,2,300,150,60,90,1,-1,-1,-1\n2,1,105,198,50,80,1,-1,-1,-1\n</code></pre> <p>Fields:</p> <ul> <li><code>frame</code> \u2014 Frame number (1-indexed)</li> <li><code>id</code> \u2014 Unique object ID per track</li> <li><code>bb_left</code>, <code>bb_top</code> \u2014 Top-left bounding box corner</li> <li><code>bb_width</code>, <code>bb_height</code> \u2014 Bounding box dimensions</li> <li><code>conf</code> \u2014 Confidence score (1 for ground truth)</li> <li><code>x</code>, <code>y</code>, <code>z</code> \u2014 3D coordinates (-1 if unused)</li> </ul>"},{"location":"learn/evaluate/#directory-layouts","title":"Directory Layouts","text":"<p>The evaluator automatically detects whether you're using a flat or MOT-style structure. Both <code>gt_dir</code> and <code>tracker_dir</code> should point directly at the parent directory of the sequences.</p> MOT LayoutFlat Layout <p>Standard MOT Challenge nested structure. Point <code>gt_dir</code> at the directory that directly contains the sequence folders, and <code>tracker_dir</code> at the directory containing the <code>{seq}.txt</code> files.</p> <pre><code>gt/\n\u251c\u2500\u2500 MOT17-02-FRCNN/\n\u2502   \u2514\u2500\u2500 gt/gt.txt\n\u251c\u2500\u2500 MOT17-04-FRCNN/\n\u2502   \u2514\u2500\u2500 gt/gt.txt\n\u2514\u2500\u2500 MOT17-05-FRCNN/\n    \u2514\u2500\u2500 gt/gt.txt\n\ntrackers/\n\u251c\u2500\u2500 MOT17-02-FRCNN.txt\n\u251c\u2500\u2500 MOT17-04-FRCNN.txt\n\u2514\u2500\u2500 MOT17-05-FRCNN.txt\n</code></pre> <p>Python</p> <pre><code>from trackers.eval import evaluate_mot_sequences\n\nresult = evaluate_mot_sequences(\n    gt_dir=\"gt\",\n    tracker_dir=\"trackers\",\n)\n</code></pre> <p>CLI</p> <pre><code>trackers eval --gt-dir gt --tracker-dir trackers\n</code></pre> <p>One <code>.txt</code> file per sequence, placed directly in the directories.</p> <pre><code>gt/\n\u251c\u2500\u2500 MOT17-02-FRCNN.txt\n\u251c\u2500\u2500 MOT17-04-FRCNN.txt\n\u2514\u2500\u2500 MOT17-05-FRCNN.txt\n\ntrackers/\n\u251c\u2500\u2500 MOT17-02-FRCNN.txt\n\u251c\u2500\u2500 MOT17-04-FRCNN.txt\n\u2514\u2500\u2500 MOT17-05-FRCNN.txt\n</code></pre> <p>Python</p> <pre><code>from trackers.eval import evaluate_mot_sequences\n\nresult = evaluate_mot_sequences(\n    gt_dir=\"gt\",\n    tracker_dir=\"trackers\",\n)\n</code></pre> <p>CLI</p> <pre><code>trackers eval --gt-dir gt --tracker-dir trackers\n</code></pre>"},{"location":"learn/evaluate/#multi-sequence-evaluation","title":"Multi-Sequence Evaluation","text":"<p>Run evaluation across many sequences and get both per-sequence results and a combined aggregate.</p> CLIPython <pre><code>trackers eval \\\n    --gt-dir data/gt \\\n    --tracker-dir data/trackers \\\n    --metrics CLEAR HOTA Identity \\\n    --columns MOTA HOTA IDF1 \\\n    --output results.json\n</code></pre> <pre><code>from trackers.eval import evaluate_mot_sequences\n\nresult = evaluate_mot_sequences(\n    gt_dir=\"data/gt\",\n    tracker_dir=\"data/trackers\",\n    metrics=[\"CLEAR\", \"HOTA\", \"Identity\"],\n)\n\nprint(result.table(columns=[\"MOTA\", \"HOTA\", \"IDF1\"]))\n</code></pre> <p>Output:</p> <pre><code>Sequence                        MOTA    HOTA    IDF1\n----------------------------------------------------\nMOT17-02-FRCNN                75.600  62.300  72.100\nMOT17-04-FRCNN                78.200  65.100  74.800\nMOT17-05-FRCNN                71.300  59.800  69.200\n----------------------------------------------------\nCOMBINED                      75.033  62.400  72.033\n</code></pre>"},{"location":"learn/install/","title":"Install <code>trackers</code>","text":"<p>Get up and running with <code>trackers</code> in minutes. Choose your preferred package manager and start tracking objects in video.</p> <p>What you'll learn:</p> <ul> <li>Install <code>trackers</code> with <code>pip</code> or <code>uv</code></li> <li>Set up a development environment</li> </ul> <p>Requirements</p> <p>Python <code>3.10</code> or higher is required.</p>"},{"location":"learn/install/#quickstart","title":"Quickstart","text":"pipuvFrom Source <pre><code>pip install trackers\n</code></pre> <pre><code>uv pip install trackers\n</code></pre> <p>For <code>uv</code>-managed projects:</p> <pre><code>uv add trackers\n</code></pre> <p>Install the latest development version:</p> <pre><code>pip install https://github.com/roboflow/trackers/archive/refs/heads/develop.zip\n</code></pre> <p>Verify installation:</p> <pre><code>python -c \"import trackers; print(trackers.__version__)\"\n</code></pre>"},{"location":"learn/install/#development-setup","title":"Development Setup","text":"<p>Set up a local environment for contributing or modifying <code>trackers</code>.</p> virtualenvuv <pre><code># Clone and enter repository\ngit clone --depth 1 -b develop https://github.com/roboflow/trackers.git\ncd trackers\n\n# Create and activate environment\npython3.10 -m venv venv\nsource venv/bin/activate\n\n# Install in editable mode\npip install --upgrade pip\npip install -e \".\"\n</code></pre> <pre><code># Clone and enter repository\ngit clone --depth 1 -b develop https://github.com/roboflow/trackers.git\ncd trackers\n\n# Set up environment\nuv python pin 3.10\nuv sync\nuv pip install -e . --all-extras\n</code></pre> <p>Verify dev install:</p> <pre><code>python -c \"import trackers; print(trackers.__version__)\"\n</code></pre>"},{"location":"learn/track/","title":"Track Objects","text":"<p>Combine object detection with multi-object tracking to follow objects through video sequences, maintaining consistent IDs even through occlusions and fast motion.</p> <p>What you'll learn:</p> <ul> <li>Run tracking from the command line with a single command</li> <li>Configure detection models and tracking algorithms</li> <li>Visualize results with bounding boxes, IDs, and trajectories</li> <li>Build custom tracking pipelines in Python</li> </ul>"},{"location":"learn/track/#install","title":"Install","text":"<p>Use the base install for tracking with your own detector. The <code>detection</code> extra adds <code>inference-models</code> for built-in detection.</p> <pre><code>pip install trackers\n</code></pre> <pre><code>pip install trackers[detection]\n</code></pre> <p>For more options, see the install guide.</p>"},{"location":"learn/track/#quickstart","title":"Quickstart","text":"<p>Read frames from video files, webcams, RTSP streams, or image directories. Each frame flows through detection to find objects, then through tracking to assign IDs.</p> CLIPython <p>Track objects with one command. Uses RF-DETR Nano and ByteTrack by default.</p> <pre><code>trackers track --source source.mp4 --output output.mp4\n</code></pre> <p>While <code>trackers</code> focuses on ID assignment, this example uses <code>inference-models</code> for detection and <code>supervision</code> for format conversion to demonstrate end-to-end usage.</p> <pre><code>import cv2\n\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(\"rfdetr-nano\")\ntracker = ByteTrackTracker()\n\ncap = cv2.VideoCapture(\"source.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n</code></pre>"},{"location":"learn/track/#cli-command-builder","title":"CLI Command Builder","text":"<p>Generate a production-ready <code>trackers track</code> command in seconds. Use interactive controls to tune core settings without memorizing flags.</p>"},{"location":"learn/track/#trackers","title":"Trackers","text":"<p>Trackers assign stable IDs to detections across frames, maintaining object identity through motion and occlusion.</p> CLIPython <p>Select a tracker with <code>--tracker</code> and tune its behavior with <code>--tracker.*</code> arguments.</p> <pre><code>trackers track --source source.mp4 --tracker bytetrack \\\n    --tracker.lost_track_buffer 60 \\\n    --tracker.minimum_consecutive_frames 5\n</code></pre> <p> Argument Description Default <code>--tracker</code> Tracking algorithm. Options: <code>bytetrack</code>, <code>sort</code>. <code>bytetrack</code> <code>--tracker.lost_track_buffer</code> Frames to retain a track without detections. Higher values improve occlusion handling but risk ID drift. <code>30</code> <code>--tracker.track_activation_threshold</code> Minimum confidence to start a new track. Lower values catch more objects but increase false positives. <code>0.25</code> <code>--tracker.minimum_consecutive_frames</code> Consecutive detections required before a track is confirmed. Suppresses spurious detections. <code>3</code> <code>--tracker.minimum_iou_threshold</code> Minimum IoU overlap to match a detection to an existing track. Higher values require tighter alignment. <code>0.3</code> </p> <p>Customize the tracker by passing parameters to the constructor, then call <code>update()</code> each frame and <code>reset()</code> between videos.</p> <pre><code>import cv2\n\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(\"rfdetr-nano\")\ntracker = ByteTrackTracker(\n    lost_track_buffer=60,\n    minimum_consecutive_frames=5,\n)\n\ncap = cv2.VideoCapture(\"source.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n</code></pre>"},{"location":"learn/track/#detectors","title":"Detectors","text":"<p>Trackers don't detect objects\u2014they link detections across frames. A detection or segmentation model provides per-frame bounding boxes or masks that the tracker uses to assign and maintain IDs.</p> CLIPython <p>Configure detection with <code>--model.*</code> arguments. Filter by confidence and class before tracking.</p> <pre><code>trackers track --source source.mp4 --model rfdetr-medium \\\n    --model.confidence 0.3 \\\n    --model.device cuda \\\n    --classes person,car\n</code></pre> <p> Argument Description Default <code>--model</code> Model identifier. Pretrained: <code>rfdetr-nano</code>, <code>rfdetr-small</code>, <code>rfdetr-medium</code>, <code>rfdetr-large</code>. Segmentation: <code>rfdetr-seg-*</code>. <code>rfdetr-nano</code> <code>--model.confidence</code> Minimum confidence threshold. Lower values increase recall but may add noise. <code>0.5</code> <code>--model.device</code> Compute device. Options: <code>auto</code>, <code>cpu</code>, <code>cuda</code>, <code>cuda:0</code>, <code>mps</code>. <code>auto</code> <code>--classes</code> Comma-separated class names or IDs to track. Example: <code>person,car</code> or <code>0,2</code>. all <code>--model.api_key</code> Roboflow API key for custom hosted models. none </p> <p>Trackers are modular\u2014combine any detection library with any tracker. This example uses <code>inference</code> with RF-DETR.</p> <pre><code>import cv2\n\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(\"rfdetr-nano\")\ntracker = ByteTrackTracker()\n\ncap = cv2.VideoCapture(\"source.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame, confidence=0.3)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n</code></pre>"},{"location":"learn/track/#visualization","title":"Visualization","text":"<p>Visualization renders tracking results for debugging, demos, and qualitative evaluation.</p> CLIPython <p>Enable display and annotation options to see results in real time or in saved video.</p> <pre><code>trackers track --source source.mp4 --display \\\n    --show-labels --show-confidence --show-trajectories\n</code></pre> <p> Argument Description Default <code>--display</code> Opens a live preview window. Press <code>q</code> or <code>ESC</code> to quit. <code>false</code> <code>--show-boxes</code> Draw bounding boxes around tracked objects. <code>true</code> <code>--show-masks</code> Draw segmentation masks. Only available with <code>rfdetr-seg-*</code> models. <code>false</code> <code>--show-confidence</code> Show detection confidence scores in labels. <code>false</code> <code>--show-labels</code> Show class names in labels. <code>false</code> <code>--show-ids</code> Show tracker IDs in labels. <code>true</code> <code>--show-trajectories</code> Draw motion trails showing recent positions of each track. <code>false</code> </p> <p>Use <code>supervision</code> annotators to draw results on frames before saving or displaying.</p> <pre><code>import cv2\n\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(\"rfdetr-nano\")\ntracker = ByteTrackTracker()\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\ncap = cv2.VideoCapture(\"source.mp4\")\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n\n    frame = box_annotator.annotate(frame, detections)\n    frame = label_annotator.annotate(frame, detections)\n</code></pre>"},{"location":"learn/track/#source","title":"Source","text":"<p><code>trackers</code> accepts video files, webcams, RTSP streams, and directories of images as input sources.</p> CLIPython <p>Pass videos, webcams, streams, or image directories as input.</p> <pre><code>trackers track --source source.mp4\n</code></pre> <p> Argument Description Default <code>--source</code> Input source. Accepts file paths (<code>.mp4</code>, <code>.avi</code>), device indices (<code>0</code>, <code>1</code>), stream URLs (<code>rtsp://</code>), or image directories. \u2014 </p> <p>Use <code>opencv-python</code>'s <code>VideoCapture</code> to read frames from files, webcams, or streams.</p> <pre><code>import cv2\n\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(\"rfdetr-nano\")\ntracker = ByteTrackTracker()\n\ncap = cv2.VideoCapture(0)\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n</code></pre>"},{"location":"learn/track/#output","title":"Output","text":"<p>Save tracking results as annotated video files or display them in real time.</p> CLIPython <p>Specify an output path to save annotated video.</p> <pre><code>trackers track --source source.mp4 --output output.mp4 --overwrite\n</code></pre> <p> Argument Description Default <code>--output</code> Path for output video. If a directory is given, saves as <code>output.mp4</code> inside it. none <code>--overwrite</code> Allow overwriting existing output files. Without this flag, existing files cause an error. <code>false</code> </p> <p>Use <code>opencv-python</code>'s <code>VideoWriter</code> to save annotated frames with full control over codec and frame rate.</p> <pre><code>import cv2\n\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrackTracker\n\nmodel = get_model(\"rfdetr-nano\")\ntracker = ByteTrackTracker()\nbox_annotator = sv.BoxAnnotator()\n\ncap = cv2.VideoCapture(\"source.mp4\")\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = cap.get(cv2.CAP_PROP_FPS)\n\nfourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\nout = cv2.VideoWriter(\"output.mp4\", fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n\n    frame = box_annotator.annotate(frame, detections)\n    out.write(frame)\n\ncap.release()\nout.release()\n</code></pre>"},{"location":"trackers/bytetrack/","title":"ByteTrack","text":""},{"location":"trackers/bytetrack/#overview","title":"Overview","text":"<p>ByteTrack builds on the same Kalman filter plus Hungarian algorithm framework as SORT but changes the data association strategy to use almost every detection box regardless of confidence score. It runs a two-stage matching: first match high-confidence detections to tracks, then match low-confidence detections to any unmatched tracks using IoU. This reduces missed tracks and fragmentation for occluded or weak detections while retaining simplicity and high frame rates. ByteTrack has set state-of-the-art results on standard MOT benchmarks with real-time performance, because it recovers valid low-score detections instead of discarding them.</p>"},{"location":"trackers/bytetrack/#comparison","title":"Comparison","text":"<p>For comparisons with other trackers, plus dataset context and evaluation details, see the tracker comparison page.</p> Dataset HOTA IDF1 MOTA MOT17 60.1 73.2 74.1 SportsMOT 73.0 72.5 96.4 SoccerNet 84.0 78.1 97.8"},{"location":"trackers/bytetrack/#run-on-video-webcam-or-rtsp-stream","title":"Run on video, webcam, or RTSP stream","text":"<p>These examples use <code>opencv-python</code> for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code>, <code>&lt;WEBCAM_INDEX&gt;</code>, and <code>&lt;RTSP_STREAM_URL&gt;</code> with your inputs. <code>&lt;WEBCAM_INDEX&gt;</code> is usually 0 for the default camera.</p> VideoWebcamRTSP <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;WEBCAM_INDEX&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open webcam\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;RTSP_STREAM_URL&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open RTSP stream\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"trackers/comparison/","title":"Tracker Comparison","text":"<p>This page shows head-to-head performance of SORT and ByteTrack on standard MOT benchmarks.</p>"},{"location":"trackers/comparison/#mot17","title":"MOT17","text":"<p>Pedestrian tracking with crowded scenes and frequent occlusions. Strongly tests re-identification and identity stability.</p> Tracker HOTA IDF1 MOTA SORT 58.4 69.9 67.2 ByteTrack 60.1 73.2 74.1"},{"location":"trackers/comparison/#sportsmot","title":"SportsMOT","text":"<p>Sports broadcast tracking with fast motion, camera pans, and similar-looking targets. Tests association under speed and appearance ambiguity.</p> Tracker HOTA IDF1 MOTA SORT 70.9 68.9 95.7 ByteTrack 73.0 72.5 96.4"},{"location":"trackers/comparison/#soccernet-tracking","title":"SoccerNet-tracking","text":"<p>Long sequences with dense interactions and partial occlusions. Tests long-term ID consistency.</p> Tracker HOTA IDF1 MOTA SORT 81.6 76.2 95.1 ByteTrack 84.0 78.1 97.8"},{"location":"trackers/sort/","title":"SORT","text":""},{"location":"trackers/sort/#overview","title":"Overview","text":"<p>SORT is a classic online, tracking-by-detection method that predicts object motion with a Kalman filter and matches predicted tracks to detections using the Hungarian algorithm based on Intersection over Union (IoU). The tracker uses only geometric cues from bounding boxes, without appearance features, so it runs extremely fast and scales to hundreds of frames per second on typical hardware. Detections from a strong CNN detector feed SORT, which updates each track\u2019s state via a constant velocity motion model and prunes stale tracks. Because SORT lacks explicit re-identification or appearance cues, it can suffer identity switches and fragmented tracks under long occlusions or heavy crowding.</p>"},{"location":"trackers/sort/#comparison","title":"Comparison","text":"<p>For comparisons with other trackers, plus dataset context and evaluation details, see the tracker comparison page.</p> Dataset HOTA IDF1 MOTA MOT17 58.4 69.9 67.2 SportsMOT 70.9 68.9 95.7 SoccerNet 81.6 76.2 95.1"},{"location":"trackers/sort/#run-on-video-webcam-or-rtsp-stream","title":"Run on video, webcam, or RTSP stream","text":"<p>These examples use <code>opencv-python</code> for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code>, <code>&lt;WEBCAM_INDEX&gt;</code>, and <code>&lt;RTSP_STREAM_URL&gt;</code> with your inputs. <code>&lt;WEBCAM_INDEX&gt;</code> is usually 0 for the default camera.</p> VideoWebcamRTSP <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;WEBCAM_INDEX&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open webcam\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;RTSP_STREAM_URL&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open RTSP stream\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(\n        annotated_frame,\n        detections,\n        labels=detections.tracker_id,\n    )\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"}]}