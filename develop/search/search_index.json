{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p><code>trackers</code> is a unified library offering clean room re-implementations of leading multi-object tracking algorithms. Its modular design allows you to easily swap trackers and integrate them with object detectors from various libraries like <code>inference</code>, <code>ultralytics</code>, or <code>transformers</code>.</p> Tracker Paper MOTA Year Status Colab SORT 74.6 2016 \u2705 DeepSORT 75.4 2017 \u2705 ByteTrack 77.8 2021 \ud83d\udea7 \ud83d\udea7 OC-SORT 75.9 2022 \ud83d\udea7 \ud83d\udea7 BoT-SORT 77.8 2022 \ud83d\udea7 \ud83d\udea7"},{"location":"#installation","title":"Installation","text":"<p>You can install <code>trackers</code> in a Python&gt;=3.9 environment.</p> <p>Basic Installation</p> pippoetryuv <pre><code>pip install trackers\n</code></pre> <pre><code>poetry add trackers\n</code></pre> <pre><code>uv pip install trackers\n</code></pre> <p>Hardware Acceleration</p> CPUCUDA 11.8CUDA 12.4CUDA 12.6ROCm 6.1ROCm 6.2.4 <pre><code>pip install \"trackers[cpu]\"\n</code></pre> <pre><code>pip install \"trackers[cu118]\"\n</code></pre> <pre><code>pip install \"trackers[cu124]\"\n</code></pre> <pre><code>pip install \"trackers[cu126]\"\n</code></pre> <pre><code>pip install \"trackers[rocm61]\"\n</code></pre> <pre><code>pip install \"trackers[rocm624]\"\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>With a modular design, <code>trackers</code> lets you combine object detectors from different libraries with the tracker of your choice. Here's how you can use <code>SORTTracker</code> with various detectors:</p> inferencerf-detrultralyticstransformers <pre><code>import supervision as sv\nfrom trackers import SORTTracker\nfrom inference import get_model\n\ntracker = SORTTracker()\nmodel = get_model(model_id=\"yolov11m-640\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import supervision as sv\nfrom trackers import SORTTracker\nfrom rfdetr import RFDETRBase\n\ntracker = SORTTracker()\nmodel = RFDETRBase()\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    detections = model.predict(frame)\n    detections = tracker.update(detections)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import supervision as sv\nfrom trackers import SORTTracker\nfrom ultralytics import YOLO\n\ntracker = SORTTracker()\nmodel = YOLO(\"yolo11m.pt\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    result = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(result)\n    detections = tracker.update(detections)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom trackers import SORTTracker\nfrom transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n\ntracker = SORTTracker()\nprocessor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\nmodel = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    inputs = processor(images=frame, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    h, w, _ = frame.shape\n    results = processor.post_process_object_detection(\n        outputs,\n        target_sizes=torch.tensor([(h, w)]),\n        threshold=0.5\n    )[0]\n\n    detections = sv.Detections.from_transformers(\n        transformers_results=results,\n        id2label=model.config.id2label\n    )\n\n    detections = tracker.update(detections)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre>"},{"location":"trackers/core/deepsort/tracker/","title":"DeepSORT","text":""},{"location":"trackers/core/deepsort/tracker/#overview","title":"Overview","text":"<p>DeepSORT extends the original SORT algorithm by integrating appearance information through a deep association metric. While maintaining the core Kalman filtering and Hungarian algorithm components from SORT, DeepSORT adds a convolutional neural network (CNN) trained on large-scale person re-identification datasets to extract appearance features from detected objects. This integration allows the tracker to maintain object identities through longer periods of occlusion, effectively reducing identity switches compared to the original SORT. DeepSORT operates with a dual-metric approach, combining motion information (Mahalanobis distance) with appearance similarity (cosine distance in feature space) to improve data association decisions. It also introduces a matching cascade that prioritizes recently seen tracks, enhancing robustness during occlusions. Most of the computational complexity is offloaded to an offline pre-training stage, allowing the online tracking component to run efficiently at approximately 20Hz, making it suitable for real-time applications while achieving competitive tracking performance with significantly improved identity preservation.</p>"},{"location":"trackers/core/deepsort/tracker/#examples","title":"Examples","text":"inferencerf-detrultralyticstransformers <pre><code>import supervision as sv\nfrom trackers import DeepSORTTracker, ReIDModel\nfrom inference import get_model\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\ntracker = DeepSORTTracker(reid_model=reid_model)\nmodel = get_model(model_id=\"yolov11m-640\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections, frame)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import supervision as sv\nfrom trackers import DeepSORTTracker, ReIDModel\nfrom rfdetr import RFDETRBase\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\ntracker = DeepSORTTracker(reid_model=reid_model)\nmodel = RFDETRBase()\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    detections = model.predict(frame)\n    detections = tracker.update(detections, frame)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import supervision as sv\nfrom trackers import DeepSORTTracker, ReIDModel\nfrom ultralytics import YOLO\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\ntracker = DeepSORTTracker(reid_model=reid_model)\nmodel = YOLO(\"yolo11m.pt\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    result = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(result)\n    detections = tracker.update(detections, frame)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom trackers import DeepSORTTracker, ReIDModel\nfrom transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\ntracker = DeepSORTTracker(reid_model=reid_model)\nprocessor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\nmodel = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    inputs = processor(images=frame, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    h, w, _ = frame.shape\n    results = processor.post_process_object_detection(\n        outputs,\n        target_sizes=torch.tensor([(h, w)]),\n        threshold=0.5\n    )[0]\n\n    detections = sv.Detections.from_transformers(\n        transformers_results=results,\n        id2label=model.config.id2label\n    )\n\n    detections = tracker.update(detections, frame)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre>"},{"location":"trackers/core/deepsort/tracker/#api","title":"API","text":"<p>Install DeepSORT</p> CPUCUDA 11.8CUDA 12.4CUDA 12.6ROCm 6.1ROCm 6.2.4 <pre><code>pip install \"trackers[reid,cpu]\"\n</code></pre> <pre><code>pip install \"trackers[reid,cu118]\"\n</code></pre> <pre><code>pip install \"trackers[reid,cu124]\"\n</code></pre> <pre><code>pip install \"trackers[reid,cu126]\"\n</code></pre> <pre><code>pip install \"trackers[reid,rocm61]\"\n</code></pre> <pre><code>pip install \"trackers[reid,rocm624]\"\n</code></pre>"},{"location":"trackers/core/deepsort/tracker/#trackers.core.deepsort.tracker.DeepSORTTracker","title":"<code>trackers.core.deepsort.tracker.DeepSORTTracker</code>","text":"<p>               Bases: <code>BaseTrackerWithFeatures</code></p> <p>Implements DeepSORT (Deep Simple Online and Realtime Tracking).</p> <p>DeepSORT extends SORT by integrating appearance information using a deep learning model, improving tracking through occlusions and reducing ID switches. It combines motion (Kalman filter) and appearance cues for data association.</p> <p>Parameters:</p> Name Type Description Default <code>reid_model</code> <code>ReIDModel</code> <p>An instance of a <code>ReIDModel</code> to extract appearance features.</p> required <code>device</code> <code>Optional[str]</code> <p>Device to run the feature extraction model on (e.g., 'cpu', 'cuda').</p> <code>None</code> <code>lost_track_buffer</code> <code>int</code> <p>Number of frames to buffer when a track is lost. Enhances occlusion handling but may increase ID switches for similar objects.</p> <code>30</code> <code>frame_rate</code> <code>float</code> <p>Frame rate of the video (frames per second). Used to calculate the maximum time a track can be lost.</p> <code>30.0</code> <code>track_activation_threshold</code> <code>float</code> <p>Detection confidence threshold for track activation. Higher values reduce false positives but might miss objects.</p> <code>0.25</code> <code>minimum_consecutive_frames</code> <code>int</code> <p>Number of consecutive frames an object must be tracked to be considered 'valid'. Prevents spurious tracks but may miss short tracks.</p> <code>3</code> <code>minimum_iou_threshold</code> <code>float</code> <p>IOU threshold for gating in the matching cascade.</p> <code>0.3</code> <code>appearance_threshold</code> <code>float</code> <p>Cosine distance threshold for appearance matching. Only matches below this threshold are considered valid.</p> <code>0.7</code> <code>appearance_weight</code> <code>float</code> <p>Weight (0-1) balancing motion (IOU) and appearance distance in the combined matching cost.</p> <code>0.5</code> <code>distance_metric</code> <code>str</code> <p>Distance metric for appearance features (e.g., 'cosine', 'euclidean'). See <code>scipy.spatial.distance.cdist</code>.</p> <code>'cosine'</code>"},{"location":"trackers/core/deepsort/tracker/#trackers.core.deepsort.tracker.DeepSORTTracker.update","title":"<code>update(detections, frame)</code>","text":"<p>Updates the tracker state with new detections and appearance features.</p> <p>Extracts appearance features, performs Kalman filter prediction, calculates IOU and appearance distance matrices, associates detections with tracks using a combined metric, updates matched tracks (position and appearance), and initializes new tracks for unmatched high-confidence detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The latest set of object detections.</p> required <code>frame</code> <code>ndarray</code> <p>The current video frame, used for extracting appearance features from detections.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>sv.Detections: A copy of the input detections, augmented with assigned <code>tracker_id</code> for each successfully tracked object. Detections not associated with a track will not have a <code>tracker_id</code>.</p>"},{"location":"trackers/core/deepsort/tracker/#trackers.core.deepsort.tracker.DeepSORTTracker.reset","title":"<code>reset()</code>","text":"<p>Resets the tracker's internal state.</p> <p>Clears all active tracks and resets the track ID counter.</p>"},{"location":"trackers/core/reid/reid/","title":"Re-Identification (ReID)","text":"<p>Re-identification (ReID) enables object tracking systems to recognize the same object or identity across different frames\u2014even when occlusion, appearance changes, or re-entries occur. This is essential for robust, long-term multi-object tracking.</p>"},{"location":"trackers/core/reid/reid/#installation","title":"Installation","text":"<p>To use ReID features in the trackers library, install the package with the appropriate dependencies for your hardware:</p> <p>Install trackers with ReID support</p> CPUCUDA 11.8CUDA 12.4CUDA 12.6ROCm 6.1ROCm 6.2.4 <pre><code>pip install \"trackers[reid,cpu]\"\n</code></pre> <pre><code>pip install \"trackers[reid,cu118]\"\n</code></pre> <pre><code>pip install \"trackers[reid,cu124]\"\n</code></pre> <pre><code>pip install \"trackers[reid,cu126]\"\n</code></pre> <pre><code>pip install \"trackers[reid,rocm61]\"\n</code></pre> <pre><code>pip install \"trackers[reid,rocm624]\"\n</code></pre>"},{"location":"trackers/core/reid/reid/#reidmodel","title":"ReIDModel","text":"<p>The <code>ReIDModel</code> class provides a flexible interface to extract appearance features from object detections, which can be used by trackers to associate identities across frames.</p>"},{"location":"trackers/core/reid/reid/#loading-a-reidmodel","title":"Loading a ReIDModel","text":"<p>You can initialize a <code>ReIDModel</code> from any supported pretrained model in the <code>timm</code> library using the <code>from_timm</code> method.</p> <pre><code>from trackers import ReIDModel\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\n</code></pre>"},{"location":"trackers/core/reid/reid/#supported-models","title":"Supported Models","text":"<p>The <code>ReIDModel</code> supports all models available in the timm library. You can list available models using:</p> <pre><code>import timm\nprint(timm.list_models())\n</code></pre>"},{"location":"trackers/core/reid/reid/#extracting-embeddings","title":"Extracting Embeddings","text":"<p>To extract embeddings (feature vectors) from detected objects in an image frame, use the <code>extract_features</code> method. It crops each detected bounding box from the frame, applies necessary transforms, and passes the crops through the backbone model:</p> <pre><code>import cv2\nimport supervision as sv\nfrom trackers import ReIDModel\nfrom inference import get_model\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\nmodel = get_model(model_id=\"yolov11m-640\")\n\nimage = cv2.imread(\"&lt;INPUT_IMAGE_PATH&gt;\")\n\nresult = model.infer(image)[0]\ndetections = sv.Detections.from_inference(result)\nfeatures = reid_model.extract_features(image, detections)\n</code></pre>"},{"location":"trackers/core/reid/reid/#tracking-integration","title":"Tracking Integration","text":"<p>ReID models are integrated into trackers like DeepSORT to improve identity association by providing appearance features alongside motion cues.</p> <pre><code>import supervision as sv\nfrom trackers import DeepSORTTracker, ReIDModel\nfrom inference import get_model\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\ntracker = DeepSORTTracker(reid_model=reid_model)\nmodel = get_model(model_id=\"yolov11m-640\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections, frame)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <p>This setup extracts appearance embeddings for detected objects and uses them in the tracker to maintain consistent IDs across frames.</p>"},{"location":"trackers/core/reid/reid/#training","title":"Training","text":"<p>You can train a custom ReID model using the <code>TripletsDataset</code> class, which provides triplets of anchor, positive, and negative samples for metric learning.</p> <p>Fine-tuning a pre-trained ReID model or training one from scratch can be beneficial when:</p> <ul> <li> <p>Your target domain (specific camera angles, lighting, object appearances) differs significantly from the data the pre-trained model was exposed to.</p> </li> <li> <p>You have a custom dataset featuring unique identities or appearance variations not covered by generic models.</p> </li> <li> <p>You aim to boost performance for specific tracking scenarios where general models might underperform. This allows the model to learn features more specific to your data.</p> </li> </ul>"},{"location":"trackers/core/reid/reid/#dataset-structure","title":"Dataset Structure","text":"<p>Prepare your dataset with the following directory structure, where each subfolder represents a unique identity:</p> <pre><code>root/\n\u251c\u2500\u2500 identity_1/\n\u2502   \u251c\u2500\u2500 image_1.png\n\u2502   \u251c\u2500\u2500 image_2.png\n\u2502   \u2514\u2500\u2500 image_3.png\n\u251c\u2500\u2500 identity_2/\n\u2502   \u251c\u2500\u2500 image_1.png\n\u2502   \u251c\u2500\u2500 image_2.png\n\u2502   \u2514\u2500\u2500 image_3.png\n\u251c\u2500\u2500 identity_3/\n\u2502   \u251c\u2500\u2500 image_1.png\n\u2502   \u251c\u2500\u2500 image_2.png\n\u2502   \u2514\u2500\u2500 image_3.png\n...\n</code></pre> <p>Each folder contains images of the same object or person under different conditions.</p> <pre><code>from torch.utils.data import DataLoader\nfrom trackers.core.reid.dataset.base import TripletsDataset\nfrom trackers import ReIDModel\n\ntrain_dataset = TripletsDataset.from_image_directories(\n    root_directory=\"&lt;DATASET_ROOT_DIRECTORY&gt;\",\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\n\nreid_model.train(\n    train_loader,\n    epochs=10,\n    projection_dimension=len(train_dataset),\n    freeze_backbone=True,\n    learning_rate=5e-4,\n    weight_decay=1e-2,\n    checkpoint_interval=5,\n)\n</code></pre>"},{"location":"trackers/core/reid/reid/#metrics-and-monitoring","title":"Metrics and Monitoring","text":"<p>During training, the model monitors metrics such as triplet loss and triplet accuracy to evaluate embedding quality.</p> <ul> <li> <p>Triplet Loss: Encourages embeddings of the same identity to be close and different identities to be far apart.</p> </li> <li> <p>Triplet Accuracy: Measures how often the model correctly ranks positive samples closer than negatives.</p> </li> </ul> <p>You can enable logging to various backends (matplotlib, TensorBoard, Weights &amp; Biases) during training for real-time monitoring:</p> <pre><code>from torch.utils.data import DataLoader\nfrom trackers.core.reid.dataset.base import TripletsDataset\nfrom trackers import ReIDModel\n\ntrain_dataset = TripletsDataset.from_image_directories(\n    root_directory=\"&lt;DATASET_ROOT_DIRECTORY&gt;\",\n)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n\nreid_model = ReIDModel.from_timm(\"resnetv2_50.a1h_in1k\")\n\nreid_model.train(\n    train_loader,\n    epochs=10,\n    projection_dimension=len(train_dataset),\n    freeze_backbone=True,\n    learning_rate=5e-4,\n    weight_decay=1e-2,\n    checkpoint_interval=5,\n    log_to_matplotlib=True,\n    log_to_tensorboard=True,\n    log_to_wandb=True,\n)\n</code></pre> <p>To use the logging capabilities for Matplotlib, TensorBoard, or Weights &amp; Biases, you might need to install additional dependencies.</p> <pre><code>pip install \"trackers[metrics]\"\n</code></pre>"},{"location":"trackers/core/reid/reid/#resuming-from-checkpoints","title":"Resuming from Checkpoints","text":"<p>You can load custom-trained weights or resume training from a checkpoint:</p> <pre><code>from trackers import ReIDModel\n\nreid_model = ReIDModel.from_timm(\"&lt;PATH_TO_CUSTOM_SAFETENSORS_CHECKPOINT&gt;\")\n</code></pre>"},{"location":"trackers/core/reid/reid/#api","title":"API","text":""},{"location":"trackers/core/reid/reid/#trackers.core.reid.model.ReIDModel","title":"<code>trackers.core.reid.model.ReIDModel</code>","text":"<p>A ReID model that is used to extract features from detection crops for trackers that utilize appearance features.</p> <p>Parameters:</p> Name Type Description Default <code>backbone_model</code> <code>Module</code> <p>The torch model to use as the backbone.</p> required <code>device</code> <code>Optional[str]</code> <p>The device to run the model on.</p> <code>'auto'</code> <code>transforms</code> <code>Optional[Union[Callable, list[Callable]]]</code> <p>The transforms to apply to the input images.</p> <code>None</code> <code>model_metadata</code> <code>dict[str, Any]</code> <p>Metadata about the model architecture.</p> <code>{}</code>"},{"location":"trackers/core/reid/reid/#trackers.core.reid.model.ReIDModel.from_timm","title":"<code>from_timm(model_name_or_checkpoint_path, device='auto', get_pooled_features=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>ReIDModel</code> with a timm model as the backbone.</p> <p>Parameters:</p> Name Type Description Default <code>model_name_or_checkpoint_path</code> <code>str</code> <p>Name of the timm model to use or path to a safetensors checkpoint. If the exact model name is not found, the closest match from <code>timm.list_models</code> will be used.</p> required <code>device</code> <code>str</code> <p>Device to run the model on.</p> <code>'auto'</code> <code>get_pooled_features</code> <code>bool</code> <p>Whether to get the pooled features from the model or not.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>timm.create_model</code>.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ReIDModel</code> <code>ReIDModel</code> <p>A new instance of <code>ReIDModel</code>.</p>"},{"location":"trackers/core/reid/reid/#trackers.core.reid.model.ReIDModel.extract_features","title":"<code>extract_features(detections, frame)</code>","text":"<p>Extract features from detection crops in the frame.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>Detections from which to extract features.</p> required <code>frame</code> <code>ndarray or Image</code> <p>The input frame.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Extracted features for each detection.</p>"},{"location":"trackers/core/reid/reid/#trackers.core.reid.model.ReIDModel.train","title":"<code>train(train_loader, epochs, validation_loader=None, projection_dimension=None, freeze_backbone=False, learning_rate=5e-05, weight_decay=0.0, triplet_margin=1.0, random_state=None, checkpoint_interval=None, log_dir='logs', log_to_matplotlib=False, log_to_tensorboard=False, log_to_wandb=False)</code>","text":"<p>Train/fine-tune the ReID model.</p> <p>Parameters:</p> Name Type Description Default <code>train_loader</code> <code>DataLoader</code> <p>The training data loader.</p> required <code>epochs</code> <code>int</code> <p>The number of epochs to train the model.</p> required <code>validation_loader</code> <code>Optional[DataLoader]</code> <p>The validation data loader.</p> <code>None</code> <code>projection_dimension</code> <code>Optional[int]</code> <p>The dimension of the projection layer.</p> <code>None</code> <code>freeze_backbone</code> <code>bool</code> <p>Whether to freeze the backbone of the model. The backbone is only frozen if <code>projection_dimension</code> is specified.</p> <code>False</code> <code>learning_rate</code> <code>float</code> <p>The learning rate to use for the optimizer.</p> <code>5e-05</code> <code>weight_decay</code> <code>float</code> <p>The weight decay to use for the optimizer.</p> <code>0.0</code> <code>triplet_margin</code> <code>float</code> <p>The margin to use for the triplet loss.</p> <code>1.0</code> <code>random_state</code> <code>Optional[Union[int, float, str, bytes, bytearray]]</code> <p>The random state to use for the training.</p> <code>None</code> <code>checkpoint_interval</code> <code>Optional[int]</code> <p>The interval to save checkpoints.</p> <code>None</code> <code>log_dir</code> <code>str</code> <p>The directory to save logs.</p> <code>'logs'</code> <code>log_to_matplotlib</code> <code>bool</code> <p>Whether to log to matplotlib.</p> <code>False</code> <code>log_to_tensorboard</code> <code>bool</code> <p>Whether to log to tensorboard.</p> <code>False</code> <code>log_to_wandb</code> <code>bool</code> <p>Whether to log to wandb. If <code>checkpoint_interval</code> is specified, the model will be logged to wandb as well. Project and entity name should be set using the environment variables <code>WANDB_PROJECT</code> and <code>WANDB_ENTITY</code>. For more details, refer to wandb environment variables.</p> <code>False</code>"},{"location":"trackers/core/reid/reid/#trackers.core.reid.dataset.base.TripletsDataset","title":"<code>trackers.core.reid.dataset.base.TripletsDataset</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>A dataset that provides triplets of images for training ReID models.</p> <p>This dataset is designed for training models with triplet loss, where each sample consists of an anchor image, a positive image (same identity as anchor), and a negative image (different identity from anchor).</p> <p>Parameters:</p> Name Type Description Default <code>tracker_id_to_images</code> <code>dict[str, list[str]]</code> <p>Dictionary mapping tracker IDs to lists of image paths</p> required <code>transforms</code> <code>Optional[Compose]</code> <p>Optional image transformations to apply</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>tracker_id_to_images</code> <code>dict[str, list[str]]</code> <p>Dictionary mapping tracker IDs to lists of image paths</p> <code>transforms</code> <code>Optional[Compose]</code> <p>Optional image transformations to apply</p> <code>tracker_ids</code> <code>list[str]</code> <p>List of all unique tracker IDs in the dataset</p>"},{"location":"trackers/core/reid/reid/#trackers.core.reid.dataset.base.TripletsDataset.from_image_directories","title":"<code>from_image_directories(root_directory, transforms=None, image_extensions=('.jpg', '.jpeg', '.png'))</code>  <code>classmethod</code>","text":"<p>Create TripletsDataset from a directory structured by tracker IDs.</p> <p>Parameters:</p> Name Type Description Default <code>root_directory</code> <code>str</code> <p>Root directory with tracker folders.</p> required <code>transforms</code> <code>Optional[Compose]</code> <p>Optional image transformations.</p> <code>None</code> <code>image_extensions</code> <code>Tuple[str, ...]</code> <p>Valid image extensions to load.</p> <code>('.jpg', '.jpeg', '.png')</code> <p>Returns:</p> Name Type Description <code>TripletsDataset</code> <code>TripletsDataset</code> <p>An initialized dataset.</p>"},{"location":"trackers/core/reid/reid/#trackers.core.reid.dataset.base.TripletsDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return the number of unique tracker IDs (identities) in the dataset.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The total number of unique identities (tracker IDs) available for sampling triplets.</p>"},{"location":"trackers/core/reid/reid/#trackers.core.reid.dataset.base.TripletsDataset.__getitem__","title":"<code>__getitem__(index)</code>","text":"<p>Retrieve a random triplet (anchor, positive, negative) of images for a given identity.</p> <p>For the tracker ID at the given index, samples two different images as the anchor and positive (same identity), and one image from a different tracker ID as the negative (different identity).</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Index of the tracker ID (identity) to sample the triplet from.</p> required <p>Returns:</p> Type Description <code>Tuple[Tensor, Tensor, Tensor]</code> <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: A tuple containing the anchor, positive, and negative image tensors.</p>"},{"location":"trackers/core/sort/tracker/","title":"SORT","text":""},{"location":"trackers/core/sort/tracker/#overview","title":"Overview","text":"<p>SORT (Simple Online and Realtime Tracking) is a lean, tracking-by-detection method that combines a Kalman filter for motion prediction with the Hungarian algorithm for data association. It uses object detections\u2014commonly from a high-performing CNN-based detector\u2014as its input, updating each tracked object\u2019s bounding box based on linear velocity estimates. Because SORT relies on minimal appearance modeling (only bounding box geometry is used), it is extremely fast and can run comfortably at hundreds of frames per second. This speed and simplicity make it well suited for real-time applications in robotics or surveillance, where rapid, approximate solutions are essential. However, its reliance on frame-to-frame matching makes SORT susceptible to ID switches and less robust during long occlusions, since there is no built-in re-identification module.</p>"},{"location":"trackers/core/sort/tracker/#examples","title":"Examples","text":"inferencerf-detrultralyticstransformers <pre><code>import supervision as sv\nfrom trackers import SORTTracker\nfrom inference import get_model\n\ntracker = SORTTracker()\nmodel = get_model(model_id=\"yolov11m-640\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    result = model.infer(frame)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import supervision as sv\nfrom trackers import SORTTracker\nfrom rfdetr import RFDETRBase\n\ntracker = SORTTracker()\nmodel = RFDETRBase()\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    detections = model.predict(frame)\n    detections = tracker.update(detections)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import supervision as sv\nfrom trackers import SORTTracker\nfrom ultralytics import YOLO\n\ntracker = SORTTracker()\nmodel = YOLO(\"yolo11m.pt\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    result = model(frame)[0]\n    detections = sv.Detections.from_ultralytics(result)\n    detections = tracker.update(detections)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre> <pre><code>import torch\nimport supervision as sv\nfrom trackers import SORTTracker\nfrom transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n\ntracker = SORTTracker()\nprocessor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\nmodel = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\nannotator = sv.LabelAnnotator(text_position=sv.Position.CENTER)\n\ndef callback(frame, _):\n    inputs = processor(images=frame, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    h, w, _ = frame.shape\n    results = processor.post_process_object_detection(\n        outputs,\n        target_sizes=torch.tensor([(h, w)]),\n        threshold=0.5\n    )[0]\n\n    detections = sv.Detections.from_transformers(\n        transformers_results=results,\n        id2label=model.config.id2label\n    )\n\n    detections = tracker.update(detections)\n    return annotator.annotate(frame, detections, labels=detections.tracker_id)\n\nsv.process_video(\n    source_path=\"&lt;INPUT_VIDEO_PATH&gt;\",\n    target_path=\"&lt;OUTPUT_VIDEO_PATH&gt;\",\n    callback=callback,\n)\n</code></pre>"},{"location":"trackers/core/sort/tracker/#api","title":"API","text":""},{"location":"trackers/core/sort/tracker/#trackers.core.sort.tracker.SORTTracker","title":"<code>trackers.core.sort.tracker.SORTTracker</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Implements SORT (Simple Online and Realtime Tracking).</p> <p>SORT is a pragmatic approach to multiple object tracking with a focus on simplicity and speed. It uses a Kalman filter for motion prediction and the Hungarian algorithm or simple IOU matching for data association.</p> <p>Parameters:</p> Name Type Description Default <code>lost_track_buffer</code> <code>int</code> <p>Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly improving tracking through occlusions, but may increase the possibility of ID switching for objects with similar appearance.</p> <code>30</code> <code>frame_rate</code> <code>float</code> <p>Frame rate of the video (frames per second). Used to calculate the maximum time a track can be lost.</p> <code>30.0</code> <code>track_activation_threshold</code> <code>float</code> <p>Detection confidence threshold for track activation. Only detections with confidence above this threshold will create new tracks. Increasing this threshold reduces false positives but may miss real objects with low confidence.</p> <code>0.25</code> <code>minimum_consecutive_frames</code> <code>int</code> <p>Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing <code>minimum_consecutive_frames</code> prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks. Before the tracker is considered valid, it will be assigned <code>-1</code> as its <code>tracker_id</code>.</p> <code>3</code> <code>minimum_iou_threshold</code> <code>float</code> <p>IOU threshold for associating detections to existing tracks.</p> <code>0.3</code>"},{"location":"trackers/core/sort/tracker/#trackers.core.sort.tracker.SORTTracker.update","title":"<code>update(detections)</code>","text":"<p>Updates the tracker state with new detections.</p> <p>Performs Kalman filter prediction, associates detections with existing trackers based on IOU, updates matched trackers, and initializes new trackers for unmatched high-confidence detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The latest set of object detections from a frame.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>sv.Detections: A copy of the input detections, augmented with assigned <code>tracker_id</code> for each successfully tracked object. Detections not associated with a track will not have a <code>tracker_id</code>.</p>"},{"location":"trackers/core/sort/tracker/#trackers.core.sort.tracker.SORTTracker.reset","title":"<code>reset()</code>","text":"<p>Resets the tracker's internal state.</p> <p>Clears all active tracks and resets the track ID counter.</p>"}]}