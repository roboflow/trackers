{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms released under the permissive Apache 2.0 license. You combine them with any detection model you already use.</p>"},{"location":"#install","title":"Install","text":"<p>You can install and use <code>trackers</code> in a Python&gt;=3.10 environment. For detailed installation instructions, including installing from source and setting up a local development environment, check out our install page.</p> <p>Installation</p> <p> </p> pipuv <pre><code>pip install trackers\n</code></pre> <pre><code>uv pip install trackers\n</code></pre>"},{"location":"#tutorials","title":"Tutorials","text":"<ul> <li> <p>How to Track Objects with SORT</p> <p></p> <p>End-to-end example showing how to run RF-DETR detection with the SORT tracker.</p> <p> Run Google Colab</p> </li> <li> <p>How to Track Objects with ByteTrack</p> <p></p> <p>End-to-end example showing how to run RF-DETR detection with the ByteTrack tracker.</p> <p> Run Google Colab</p> </li> </ul>"},{"location":"#tracking-algorithms","title":"Tracking Algorithms","text":"<p>Trackers gives you clean, modular re-implementations of leading multi-object tracking algorithms. The package currently supports SORT and ByteTrack. OC-SORT support is coming soon. For full results, see the benchmarks page.</p> Algorithm Trackers API MOT17 HOTA MOT17 IDF1 MOT17 MOTA SportsMOT HOTA SoccerNet HOTA SORT <code>SORTTracker</code> 58.4 69.9 67.2 70.9 81.6 ByteTrack <code>ByteTrackTracker</code> 60.1 73.2 74.1 73.0 84.0 OC-SORT <code>OCSORTTracker</code> \u2014 \u2014 \u2014 \u2014 \u2014"},{"location":"#quickstart","title":"Quickstart","text":"<p>With a modular design, Trackers lets you combine object detectors from different libraries with the tracker of your choice. Here's how you can use ByteTrack with various detectors. These examples use OpenCV for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code> with your input.</p> RF-DETRInferenceUltralyticsTransformers <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrack\n\ntracker = ByteTrack()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom inference import get_model\nfrom trackers import ByteTrack\n\ntracker = ByteTrack()\nmodel = get_model(model_id=\"rfdetr-medium\")\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    result = model.infer(frame_rgb)[0]\n    detections = sv.Detections.from_inference(result)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"Inference + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom ultralytics import YOLO\nfrom trackers import ByteTrack\n\ntracker = ByteTrack()\nmodel = YOLO(\"yolo26m.pt\")\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    result = model(frame_rgb)[0]\n    detections = sv.Detections.from_ultralytics(result)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"Ultralytics + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import torch\nimport cv2\nimport supervision as sv\nfrom trackers import ByteTrack\nfrom transformers import RTDetrImageProcessor, RTDetrV2ForObjectDetection\n\ntracker = ByteTrack()\nprocessor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\nmodel = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    inputs = processor(images=frame_rgb, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    h, w = frame_bgr.shape[:2]\n    results = processor.post_process_object_detection(\n        outputs,\n        target_sizes=torch.tensor([[h, w]]),\n        threshold=0.5\n    )[0]\n\n    detections = sv.Detections.from_transformers(\n        transformers_results=results,\n        id2label=model.config.id2label\n    )\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"Transformers + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"learn/benchmarks/","title":"Benchmarks","text":"<p>This page presents performance results of the trackers available in the <code>trackers</code> Python package on standard multiple object tracking (MOT) benchmarks.</p> <p>The goal of this page is not only to report scores, but also to help you interpret them for real world projects.</p>"},{"location":"learn/benchmarks/#datasets","title":"Datasets","text":""},{"location":"learn/benchmarks/#mot17","title":"MOT17","text":"<p>Classic pedestrian MOT benchmark based on MOT16 sequences but with significantly improved, higher-accuracy ground truth annotations. Contains crowded street scenes captured from static and slowly moving cameras, featuring frequent occlusions, moderate density (up to ~30\u201340 pedestrians), and partial camera motion in some sequences. Strong emphasis on handling short- to medium-term occlusions and re-acquiring identities after crossings or background clutter.</p> Tracker HOTA IDF1 MOTA SORT 58.4 69.9 67.2 ByteTrack 60.1 73.2 74.1"},{"location":"learn/benchmarks/#sportsmot","title":"SportsMOT","text":"<p>Large-scale MOT dataset targeting sports analysis, containing 240 video clips (basketball, football/soccer, volleyball) collected from professional matches (Olympics, NCAA, NBA). Characterized by fast and highly variable player motion, rapid camera panning/zooming, complex backgrounds, and players with very similar appearance (uniforms within team). Only players on the court/field are annotated (excluding referees, coaches, spectators), making it particularly challenging for association under speed and visual similarity.</p> Tracker HOTA IDF1 MOTA SORT 70.9 68.9 95.7 ByteTrack 73.0 72.5 96.4"},{"location":"learn/benchmarks/#soccernet-tracking","title":"SoccerNet-tracking","text":"<p>Specialized soccer MOT dataset derived from SoccerNet-tracking broadcast videos, containing 200 short 30-second clips + one full 45-minute half-time sequence. Focuses on professional soccer matches with main camera view, including players (both teams), goalkeepers, referees, and the ball; features long sequences with occlusions by players, fast directional changes, and non-linear trajectories. Particularly useful for evaluating long-term association and robustness to crowded penalty areas or midfield clusters.</p> Tracker HOTA IDF1 MOTA SORT 81.6 76.2 95.1 ByteTrack 84.0 78.1 97.8"},{"location":"learn/benchmarks/#metrics","title":"Metrics","text":""},{"location":"learn/benchmarks/#hota-higher-order-tracking-accuracy","title":"HOTA (Higher Order Tracking Accuracy)","text":"<p>HOTA is a balanced tracking metric that evaluates detection quality, localization accuracy, and identity association at the same time. The score is computed over a range of IoU thresholds, from loose to strict, which helps capture both easy and hard matching cases. Detection and association scores are then combined into a single value, giving a more complete view of tracker behavior than metrics focused on one aspect only. The metric is computed after processing the full video sequence and gives limited penalty when the same object is represented by several shorter tracks instead of one continuous track.</p> <p>Use HOTA when you need an overall assessment of tracking quality and want to compare systems under realistic conditions where both detection errors and association errors matter.</p>"},{"location":"learn/benchmarks/#idf1-identification-f1-score","title":"IDF1 (Identification F1 Score)","text":"<p>IDF1 measures how well a tracker keeps the same ID for the same object over time. It matches predicted IDs to ground truth IDs across the whole video and scores identity precision and recall. The metric reacts strongly to ID switches and lost identities over long periods. Detection coverage has little effect on the score if the remaining IDs stay consistent.</p> <p>Use IDF1 when stable track IDs matter more than full object coverage, for example trajectory analysis, player tracking, or any logic built on long lived identities.</p>"},{"location":"learn/benchmarks/#mota-multi-object-tracking-accuracy","title":"MOTA (Multi-Object Tracking Accuracy)","text":"<p>MOTA sums three types of errors. Missed objects, false detections, and ID switches. The score reflects how often a tracker makes basic mistakes frame to frame. Missed detections and false positives dominate the result. The metric does not account for box accuracy and does not reflect long term identity consistency.</p> <p>Use MOTA when detection coverage represents the main objective and identity stability plays a secondary role.</p>"},{"location":"learn/benchmarks/#reproducibility","title":"Reproducibility","text":"<p>All reported numbers were obtained using the official TrackEval library. We used YOLOX detections for MOT17 (provided by the ByteTrack authors), YOLOX detections trained on SportsMOT (provided by the SportsMOT dataset authors), and dataset-provided oracle detections (perfect detections) for SoccerNet-tracking.</p> <p>We are currently working on releasing dedicated benchmarking utilities in the <code>trackers</code> package that will allow users to easily reproduce these exact numbers, run evaluation on custom trackers using the same protocol, and benchmark their own models on these datasets with standardized splits and settings.</p> <p>Stay tuned for the upcoming release of the benchmarking module.</p>"},{"location":"learn/install/","title":"Installation","text":"<p>Welcome to Trackers! This guide will help you install and set up Trackers for your projects. Whether you're a developer looking to contribute or an end-user ready to start using Trackers, we've got you covered.</p>"},{"location":"learn/install/#installation-methods","title":"Installation Methods","text":"<p>Trackers supports several installation methods. Python 3.10 or higher is required. Choose the option which best fits your workflow.</p> <p>Installation</p> pip (recommended)uvSource Archive <p>The easiest way to install Trackers is using <code>pip</code>. This method is recommended for most users.</p> <pre><code>pip install trackers\n</code></pre> <p>If you are using <code>uv</code>, you can install Trackers using the following command.</p> <pre><code>uv pip install trackers\n</code></pre> <p>For uv-managed projects, add Trackers as a project dependency.</p> <pre><code>uv add trackers\n</code></pre> <p>To install the latest development version of Trackers from source without cloning the full repository, run the command below.</p> <pre><code>pip install https://github.com/roboflow/trackers/archive/refs/heads/develop.zip\n</code></pre>"},{"location":"learn/install/#dev-environment","title":"Dev Environment","text":"<p>If you plan to contribute to Trackers or modify the codebase locally, set up a local development environment using the steps below. We recommend using an isolated environment to avoid dependency conflicts.</p> <p>Development Setup</p> virtualenvuv <pre><code># Clone repository\ngit clone --depth 1 -b develop https://github.com/roboflow/trackers.git\ncd trackers\n\n# Create virtual environment\npython3.10 -m venv venv\n\n# Activate environment\nsource venv/bin/activate\n\n# Upgrade pip\npip install --upgrade pip\n\n# Install in editable mode\npip install -e \".\"\n</code></pre> <pre><code># Clone repository\ngit clone --depth 1 -b develop https://github.com/roboflow/trackers.git\ncd trackers\n\n# Pin Python version\nuv python pin 3.10\n\n# Sync environment and install dependencies\nuv sync\n\n# Install in editable mode with extras\nuv pip install -e . --all-extras\n</code></pre>"},{"location":"learn/install/#troubleshooting","title":"Troubleshooting","text":"<p>Installation issues usually fall into a few common categories.</p> <ul> <li>Permission Issues. Package install fails due to missing write access to system Python paths. Fix by running <code>pip install --user trackers</code> or by using an isolated environment.</li> <li>Dependency Conflicts. Errors appear after installing other Python packages. Resolve by installing Trackers inside a fresh virtual environment or a clean <code>uv</code> project.</li> <li>Python Version. Installation fails or runtime errors appear when using older Python releases. Trackers requires Python 3.10 or higher.</li> </ul> <p>If problems persist, open an issue on the GitHub repository.</p>"},{"location":"trackers/bytetrack/","title":"ByteTrack","text":""},{"location":"trackers/bytetrack/#overview","title":"Overview","text":"<p>ByteTrack builds on the same Kalman filter plus Hungarian algorithm framework as SORT but changes the data association strategy to use almost every detection box regardless of confidence score. It runs a two-stage matching: first match high-confidence detections to tracks, then match low-confidence detections to any unmatched tracks using IoU. This reduces missed tracks and fragmentation for occluded or weak detections while retaining simplicity and high frame rates. ByteTrack has set state-of-the-art results on standard MOT benchmarks with real-time performance, because it recovers valid low-score detections instead of discarding them.</p>"},{"location":"trackers/bytetrack/#benchmarks","title":"Benchmarks","text":"<p>For comparisons with other trackers, plus full details on the datasets and evaluation metrics used, see the benchmarks page.</p> Dataset HOTA IDF1 MOTA MOT17 60.1 73.2 74.1 SportsMOT 73.0 72.5 96.4 SoccerNet 84.0 78.1 97.8"},{"location":"trackers/bytetrack/#run-on-video-webcam-or-rtsp-stream","title":"Run on video, webcam, or RTSP stream","text":"<p>These examples use OpenCV for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code>, <code>&lt;WEBCAM_INDEX&gt;</code>, and <code>&lt;RTSP_STREAM_URL&gt;</code> with your inputs. <code>&lt;WEBCAM_INDEX&gt;</code> is usually 0 for the default camera.</p> VideoWebcamRTSP <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;WEBCAM_INDEX&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open webcam\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import ByteTrackTracker\n\ntracker = ByteTrackTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;RTSP_STREAM_URL&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open RTSP stream\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"RF-DETR + ByteTrack\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"trackers/bytetrack/#api","title":"API","text":""},{"location":"trackers/bytetrack/#trackers.core.bytetrack.tracker.ByteTrackTracker","title":"<code>trackers.core.bytetrack.tracker.ByteTrackTracker</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Implements ByteTrack.</p> <p>ByteTrack is a simple, effective, and generic multi-object tracking method that improves upon tracking-by-detection by associating every detection box instead of discarding low-score ones. This makes it more robust to occlusions. It uses a two-stage association process and builds on established techniques like the Kalman Filter for motion prediction and the Hungarian algorithm for data association.</p> <p>Parameters:</p> Name Type Description Default <code>lost_track_buffer</code> <code>int</code> <p>Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly improving tracking through occlusions, but may increase the possibility of ID switching for objects that disappear.</p> <code>30</code> <code>frame_rate</code> <code>float</code> <p>Frame rate of the video (frames per second). Used to calculate the maximum time a track can be lost.</p> <code>30.0</code> <code>track_activation_threshold</code> <code>float</code> <p>Detection confidence threshold for track activation. Only detections with confidence above this threshold will create new tracks. Increasing this threshold may reduce false positives but may miss real objects with low confidence.</p> <code>0.7</code> <code>minimum_consecutive_frames</code> <code>int</code> <p>Number of consecutive frames that an object must be tracked before it is considered a 'valid'/'active/ track. Increasing <code>minimum_consecutive_frames</code> prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks. Before the tracker is considered valid, it will be assigned <code>-1</code> as its <code>tracker_id</code>.</p> <code>2</code> <code>minimum_iou_threshold</code> <code>float</code> <p>IoU threshold for associating detections to existing tracks. Prevents the association of lower IoU than the threshold between boxes and tracks. A higher value will only associate boxes that have more overlapping area.</p> <code>0.1</code> <code>high_conf_det_threshold</code> <code>float</code> <p>threshold for assigning detections to high probability class. A higher value will classify only higher confidence/probability detections as 'high probability' per the ByteTrack algorithm, which are used in the first similarity step of the algorithm.</p> <code>0.6</code>"},{"location":"trackers/bytetrack/#trackers.core.bytetrack.tracker.ByteTrackTracker.update","title":"<code>update(detections)</code>","text":"<p>Updates the tracker state with new detections.</p> <p>Performs Kalman Filter prediction, associates detections with existing tracks based on IoU, updates matched tracks, and initializes new tracks for unmatched high-confidence detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The latest set of object detections from a frame.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A copy of the input detections, augmented with assigned <code>tracker_id</code> for each successfully tracked object. Detections not associated with a track will not have a <code>tracker_id</code>. The order of the detections is not guaranteed to be the same as the input detections.</p>"},{"location":"trackers/bytetrack/#trackers.core.bytetrack.tracker.ByteTrackTracker.reset","title":"<code>reset()</code>","text":"<p>Resets the tracker's internal state.</p> <p>Clears all active tracks and resets the track ID counter.</p>"},{"location":"trackers/sort/","title":"SORT","text":""},{"location":"trackers/sort/#overview","title":"Overview","text":"<p>SORT is a classic online, tracking-by-detection method that predicts object motion with a Kalman filter and matches predicted tracks to detections using the Hungarian algorithm based on Intersection over Union (IoU). The tracker uses only geometric cues from bounding boxes, without appearance features, so it runs extremely fast and scales to hundreds of frames per second on typical hardware. Detections from a strong CNN detector feed SORT, which updates each track\u2019s state via a constant velocity motion model and prunes stale tracks. Because SORT lacks explicit re-identification or appearance cues, it can suffer identity switches and fragmented tracks under long occlusions or heavy crowding.</p>"},{"location":"trackers/sort/#benchmarks","title":"Benchmarks","text":"<p>For comparisons with other trackers, plus full details on the datasets and evaluation metrics used, see the benchmarks page.</p> Dataset HOTA IDF1 MOTA MOT17 58.4 69.9 67.2 SportsMOT 70.9 68.9 95.7 SoccerNet 81.6 76.2 95.1"},{"location":"trackers/sort/#run-on-video-webcam-or-rtsp-stream","title":"Run on video, webcam, or RTSP stream","text":"<p>These examples use OpenCV for decoding and display. Replace <code>&lt;SOURCE_VIDEO_PATH&gt;</code>, <code>&lt;WEBCAM_INDEX&gt;</code>, and <code>&lt;RTSP_STREAM_URL&gt;</code> with your inputs. <code>&lt;WEBCAM_INDEX&gt;</code> is usually 0 for the default camera.</p> VideoWebcamRTSP <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;SOURCE_VIDEO_PATH&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open video source\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;WEBCAM_INDEX&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open webcam\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre> <pre><code>import cv2\nimport supervision as sv\nfrom rfdetr import RFDETRMedium\nfrom trackers import SORTTracker\n\ntracker = SORTTracker()\nmodel = RFDETRMedium()\n\nbox_annotator = sv.BoxAnnotator()\nlabel_annotator = sv.LabelAnnotator()\n\nvideo_capture = cv2.VideoCapture(\"&lt;RTSP_STREAM_URL&gt;\")\nif not video_capture.isOpened():\n    raise RuntimeError(\"Failed to open RTSP stream\")\n\nwhile True:\n    success, frame_bgr = video_capture.read()\n    if not success:\n        break\n\n    frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n    detections = model.predict(frame_rgb)\n    detections = tracker.update(detections)\n\n    annotated_frame = box_annotator.annotate(frame_bgr, detections)\n    annotated_frame = label_annotator.annotate(annotated_frame, detections, labels=detections.tracker_id)\n\n    cv2.imshow(\"RF-DETR + SORT\", annotated_frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord(\"q\"):\n        break\n\nvideo_capture.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"trackers/sort/#api","title":"API","text":""},{"location":"trackers/sort/#trackers.core.sort.tracker.SORTTracker","title":"<code>trackers.core.sort.tracker.SORTTracker</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Implements SORT (Simple Online and Realtime Tracking).</p> <p>SORT is a pragmatic approach to multiple object tracking with a focus on simplicity and speed. It uses a Kalman filter for motion prediction and the Hungarian algorithm or simple IOU matching for data association.</p> <p>Parameters:</p> Name Type Description Default <code>lost_track_buffer</code> <code>int</code> <p>Number of frames to buffer when a track is lost. Increasing lost_track_buffer enhances occlusion handling, significantly improving tracking through occlusions, but may increase the possibility of ID switching for objects with similar appearance.</p> <code>30</code> <code>frame_rate</code> <code>float</code> <p>Frame rate of the video (frames per second). Used to calculate the maximum time a track can be lost.</p> <code>30.0</code> <code>track_activation_threshold</code> <code>float</code> <p>Detection confidence threshold for track activation. Only detections with confidence above this threshold will create new tracks. Increasing this threshold reduces false positives but may miss real objects with low confidence.</p> <code>0.25</code> <code>minimum_consecutive_frames</code> <code>int</code> <p>Number of consecutive frames that an object must be tracked before it is considered a 'valid' track. Increasing <code>minimum_consecutive_frames</code> prevents the creation of accidental tracks from false detection or double detection, but risks missing shorter tracks. Before the tracker is considered valid, it will be assigned <code>-1</code> as its <code>tracker_id</code>.</p> <code>3</code> <code>minimum_iou_threshold</code> <code>float</code> <p>IOU threshold for associating detections to existing tracks.</p> <code>0.3</code>"},{"location":"trackers/sort/#trackers.core.sort.tracker.SORTTracker.update","title":"<code>update(detections)</code>","text":"<p>Updates the tracker state with new detections.</p> <p>Performs Kalman filter prediction, associates detections with existing trackers based on IOU, updates matched trackers, and initializes new trackers for unmatched high-confidence detections.</p> <p>Parameters:</p> Name Type Description Default <code>detections</code> <code>Detections</code> <p>The latest set of object detections from a frame.</p> required <p>Returns:</p> Type Description <code>Detections</code> <p>A copy of the input detections, augmented with assigned <code>tracker_id</code> for each successfully tracked object. Detections not associated with a track will not have a <code>tracker_id</code>.</p>"},{"location":"trackers/sort/#trackers.core.sort.tracker.SORTTracker.reset","title":"<code>reset()</code>","text":"<p>Resets the tracker's internal state.</p> <p>Clears all active tracks and resets the track ID counter.</p>"}]}